# Mozilla.ai Lumigator

Lumigator is an open-source platform built by [Mozilla.ai](https://www.mozilla.ai/) for guiding users through the process of selecting the right language model for their needs.
Currently, we support evaluating summarization tasks using sequence-to-sequence models like BART and BERT and causal architectures like GPT and Mistral,
but will be expanding to other machine learning tasks and use-cases.

See [example notebook](/notebooks/walkthrough.ipynb) for full platform API walkthrough.

### Available Machine Learning Tasks:

 - Summarization

### Available Models for Online Ground Truth Generation:

| Model Type | Model                                        | via HuggingFace | via API |
|------------|----------------------------------------------|-----------------|---------|
| seq2seq    | bart-large-cnn                               |       X         |         |
| causal     | gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo-0125 |                 |    X    |
| causal     | open-mistral-7b                              |                 |    X    |


### Available Models for Offline Evaluation:

| Model Type | Model                                        | via HuggingFace | via API |
|------------|----------------------------------------------|-----------------|---------|
| seq2seq    | bart-large-cnn                               |       X         |         |
| seq2seq    | longformer-qmsum-meeting-summarization       |       X         |         |
| causal     | gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo-0125 |                 |    X    |
| causal     | open-mistral-7b                              |                 |    X    |


### Available Metrics:

+ ROUGE - (Recall-Oriented Understudy for Gisting Evaluation), which compares an automatically-generated summary to one generated by a machine learning model on a score of 0 to 1 in a range of metrics comparing statistical similarity of two texts.
+ BLEU - (Bilingual Evaluation Understudy) - Also a measure between 0 and 1 that looks at the modified precision between ngrams in a candidate (ground truth) sentence and reference sentence
+ METEOR - Looks at the harmonic mean of precision and recall
+ BERTScore - Generates embeddings of ground truth input and model output and compares their cosine similarity


> [!Info]
>
> Lumigator is in the early stages of development.
> It is missing important features and documentation.
> You should expect breaking changes in the core interfaces and configuration structures
> as development continues.

# Technical Overview

Lumigator is a Python-based FastAPI web app with REST API endpoints that allow for access to services for serving and evaluating large language models available as safetensor artifacts hosted on both HuggingFace and local stores, with our first primary focus being Huggingface access, and tracking the lifecycle of a model in the backend database (Postgres).
It consists of:

+ a FastAPI-based huggingface's `evaluate` library for those metrics, but we are considering using lm-harness that manages platform activity backed by Postgres
+ online evaluation of models using **Ray Serve** deployments
+ a **Ray cluster** to run offline evaluation jobs using [lm-buddy](https://github.com/mozilla-ai/lm-buddy), our in-house eval framework
    + LM buddy spins up a vLLM deployment using Ray Serve for inference/evalution jobs and the core of evaluation happens using huggingface's `evaluate` library for those metrics, but we are evaluating lm-eval-harness, as well
+ Artifact management (S3 in the cloud, localstack locally )
+ A Postgres database to track platform-level tasks and dataset metadata

You can build and install it locally using `docker-compose` or into a distributed environment using Kubernetes `Helm charts`.

The application is built using Pants.
For more on Pants, read the [Pants guide](PANTS_GUIDE.md).

# Get Started

## Local Development Setup

Currently, developers are able to build the local project using docker-compose.

1. git clone repo
2. install development environment and pants
3. run dev setup to install docker-compose

### Install pants, tools, and dev environment.
This includes a standalone python interpreter, venv (`mzaivenv`), precommit configs, and more. Python setup is
handled by `uv`; pants maintains lockfiles for different platforms. Currently, only `python 3.11.9` is valid for this project; if a compatible interpreter
is found `uv` will not download a standalone python interpreter for you.

For VSCode users, activate the venv before opening your IDE; the `.env` file will be recognized automatically.


```shell
make bootstrap-dev-environment
source mzaivenv/bin/activate
```

Show targets:

```bash
make show-pants-targets
```

run the app locally via docker compose:

```bash
make local-up
make local-logs # gets the logs from docker compose
make local-down # shuts it down
```

Compile targets manually:

```bash
pants package <target>
# backend app
pants package lumigator/python/mzai/backend --no-local-cache
# backend docker image
pants package lumigator/python/mzai/backend:backend_image
```


## Rebuilding dependencies

You may need to manually regenerate the [lockfiles](https://www.pantsbuild.org/2.21/docs/python/overview/lockfiles) if you update dependencies.
To do so:

1. Add your new dependency to `3rdparty/python/pyproject.toml`. This file respects system platform markers, and only very special cases need to be added as explicit `python_requirement` targets.
2. run `pants generate-lockfiles`. This will take a while - 5-10 minutes in some cases and require access to pypi.

make sure to add the new lockfiles to the repo with your PR. You'll have to rebuild your dev environment if you haven't already.


## Testing the development setup

Using a container, run the following from the root of this repo:

```bash
make test-dev-setup

```
