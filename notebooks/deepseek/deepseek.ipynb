{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Evaluation of DeepSeek R1 Models for Clinical Conversation Summarization\n",
    "\n",
    "## Background and Motivation\n",
    "\n",
    "The DeepSeek family of models represents an interesting advancement in reasoning-specialized language models. While DeepSeek published evaluation results in [their paper](https://arxiv.org/pdf/2501.12948) (see Table 5), I wanted to understand specifically how the various Distilled R1 models compare to the full R1 model on a practical use case: clinical conversation summarization using the ACI-Bench dataset.\n",
    "\n",
    "This notebook demonstrates how to use **Lumigator** to systematically evaluate and compare these models. Lumigator provides a framework to:\n",
    "\n",
    "1. Coordinate multiple model evaluations against the same dataset\n",
    "2. Execute inference requests across different model deployments\n",
    "3. Calculate standardized metrics for performance comparison\n",
    "4. Organize and visualize the results for analysis\n",
    "\n",
    "## Getting Started with Lumigator\n",
    "\n",
    "To use this notebook, you'll need to have Lumigator running. In a terminal, run:\n",
    "\n",
    "```bash\n",
    "git clone git@github.com:mozilla-ai/lumigator.git\n",
    "cd lumigator\n",
    "make setup\n",
    "make start-lumigator-build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection is: OK\n"
     ]
    }
   ],
   "source": [
    "from lumigator_sdk.lumigator import LumigatorClient\n",
    "\n",
    "# Time to connect up to the Lumigator client!\n",
    "LUMI_HOST = \"localhost:8000\"\n",
    "client = LumigatorClient(api_host=LUMI_HOST)\n",
    "print(f\"Connection is: {client.health.healthcheck().status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: ACI-Bench for Clinical Documentation\n",
    "\n",
    "This evaluation uses the ACI-Bench dataset, which was introduced in the paper \n",
    "[\"ACI-Bench: a Novel Benchmark for Ambient Clinical Intelligence\"](https://www.nature.com/articles/s41597-023-02487-3) \n",
    "(Yim et al., 2023). \n",
    "\n",
    "ACI-Bench was specifically designed to evaluate AI systems on their ability to \n",
    "understand doctor-patient conversations and generate accurate clinical documentation.\n",
    "\n",
    "### About the Dataset\n",
    "\n",
    "The test split of ACI-Bench that we'll be using consists of 40 doctor-patient conversations. \n",
    "These conversations aren't from real patient encounters but were created through professional medical simulations \n",
    "with standardized patients (actors trained to portray patients) and licensed physicians. \n",
    "\n",
    "This approach attempts to keep the data reasonably realistic while also being HIPAA-compliant, \n",
    "as no actual protected health information is included.\n",
    "\n",
    "Each conversation includes:\n",
    "\n",
    "1. A full transcript of the simulated clinical encounter, with speaker identification\n",
    "2. Human-written reference documentation\n",
    "3. Various sections of the standard clinical note format (SOAP - Subjective, Objective, Assessment, Plan)\n",
    "\n",
    "### The Assessment & Plan Task\n",
    "\n",
    "In this evaluation, we're specifically working with the **assessment and plan section** (`clef_taskC_test3_assessment_and_plan.json`), which is particularly challenging as it requires:\n",
    "\n",
    "- Identifying the patient's medical conditions\n",
    "- Understanding the physician's diagnostic reasoning\n",
    "- Summarizing the recommended treatment approach\n",
    "- Capturing follow-up plans and contingencies\n",
    "\n",
    "This section of clinical documentation represents higher-level medical reasoning compared to other sections, making it a interesting test of a model's capacity for complex medical summarization and inference.\n",
    "\n",
    "Each example in our dataset contains:\n",
    "- `examples`: The full doctor-patient conversation transcript (with speaker turns marked as `[doctor]` and `[patient]`)\n",
    "- `ground_truth`: The human-written assessment and plan section that serves as the reference summary\n",
    "- `id`: A unique identifier for each conversation\n",
    "\n",
    "The Assessment & Plan task was featured in the 2023 MEDIQA-CHAT shared task at CLEF (Conference and Labs of the Evaluation Forum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of This Evaluation\n",
    "\n",
    "This evaluation has several important limitations that should be considered when interpreting the results:\n",
    "\n",
    "1. **Unknown Training Data Exposure**: We cannot verify whether DeepSeek models were trained on the ACI-Bench dataset or similar clinical conversations. If any of these models were exposed to this data during training, they would have an unfair advantage in this evaluation - essentially having already \"seen the answers\" to the test. Without model cards or detailed training information disclosing training datasets, this remains an unknown factor.\n",
    "\n",
    "2. **Relative Comparison Focus**: Given this limitation, our analysis primarily focuses on the relative performance differences between models within the DeepSeek family, rather than making absolute claims about their capabilities for clinical summarization. By comparing models from the same family, we can still draw meaningful conclusions about how performance scales with model size and architecture (Llama vs. Qwen) when all models would have had the same potential exposure to training data.\n",
    "\n",
    "3. **Single Task Evaluation**: This evaluation examines performance on just one specific clinical documentation task (Assessment & Plan generation) and may not generalize to other medical tasks or to clinical summarization in different specialties or contexts.\n",
    "\n",
    "4. **Simulated Data**: While the ACI-Bench dataset uses realistic simulated conversations, model performance might differ on real-world clinical conversations, which tend to be messier, less structured, and potentially contain more specialized terminology.\n",
    "\n",
    "5. **Zero-Shot Setting**: Our evaluation uses a zero-shot approach with a generic system prompt. \n",
    "Performance might improve significantly with few-shot examples or more specialized prompting techniques tailored to each model's capabilities.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# GitHub API URL to fetch the file list\n",
    "download_url = \"https://raw.githubusercontent.com/wyim/aci-bench/main/data/challenge_data_json/clef_taskC_test3_assessment_and_plan.json\"\n",
    "file_name = download_url.split(\"/\")[-1]\n",
    "save_dir = Path(\"data\")\n",
    "file_path = save_dir / file_name\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "response = requests.get(download_url)\n",
    "\n",
    "data = response.json()\n",
    "# convert it to a dataframe. The file by default has the columns 'src' and 'tgt'\n",
    "df = pd.DataFrame(data[\"data\"])  # noqa: PD901\n",
    "# Rename the columns to \"examples\" and \"ground_truth\", which is what the Lumigator API expects for the data\n",
    "df = df.rename(columns={\"src\": \"examples\", \"tgt\": \"ground_truth\", \"file\": \"id\"})  # noqa: PD901\n",
    "\n",
    "processed_file_path = file_path.with_suffix(\".csv\")\n",
    "# save it as a csv\n",
    "df.to_csv(processed_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now the data is all formatted: let's take a look at an example to get a feel for what the data looks like. \n",
    "Understanding the data is crucial for interpreting the results and behavior of the models being evaluated. \n",
    "\n",
    "Every dataset\n",
    "has quirks and unique things about it: in this notebook we won't dive too deeply into investigating the characteristics of the dataset,\n",
    "but it's definitely worth taking more time to understand exactly what is in a dataset before you use it for anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.iloc[0]\n",
    "print(\"--- Snippet of Conversation ---\")\n",
    "print(\"\\n\".join(sample[\"examples\"].split(\"\\n\")[6:8]))\n",
    "print(\" --- Assessment & Plan---\")\n",
    "print(sample[\"ground_truth\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Dataset into Lumigator\n",
    "Now, let's upload the dataset into lumigator using the Lumigator SDK. creating the dataset returns the dataset ID, which we will attach to future requests so that Lumigator knows which dataset should be used for running an eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset uploaded and has ID: 7c828eef-173f-4a3f-9ece-9a1093bd62f5\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from lumigator_schemas.datasets import DatasetFormat\n",
    "\n",
    "# Upload that file that we created earlier\n",
    "with Path.open(Path(processed_file_path), \"r\") as file:\n",
    "    data = file.read()\n",
    "dataset_response = client.datasets.create_dataset(dataset=data, format=DatasetFormat.JOB)\n",
    "dataset_id = dataset_response.id\n",
    "print(f\"Dataset uploaded and has ID: {dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Evaluation Pipeline in Lumigator\n",
    "\n",
    "Now that we've uploaded our dataset, we'll create an experiment in Lumigator. In Lumigator terminology:\n",
    "\n",
    "1. **Experiment** - A container that organizes related evaluation workflows\n",
    "2. **Workflow** - A specific model configuration being evaluated against the dataset\n",
    "3. **Dataset** - The collection of examples (in our case, clinical conversations)\n",
    "\n",
    "This structure allows us to compare multiple models on the same dataset in a systematic way, with all results organized within a single experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now time to create an experiment in Lumigator! This is a container for all the workflows we'll run\n",
    "from lumigator_schemas.experiments import ExperimentCreate\n",
    "\n",
    "request = ExperimentCreate(\n",
    "    name=\"ACI-Bench clef_taskC_test3_assessment_and_plan\",\n",
    "    description=\"https://github.com/wyim/aci-bench/tree/main\",\n",
    "    dataset=dataset_id,\n",
    ")\n",
    "experiment_response = client.experiments.create_experiment(request)\n",
    "experiment_id = experiment_response.id\n",
    "print(f\"Experiment created and has ID: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection Rationale\n",
    "\n",
    "For this evaluation, we're testing a range of DeepSeek models to understand how performance scales with model size and architecture:\n",
    "\n",
    "- **DeepSeek R1** - The original reasoning-specialized model\n",
    "- **DeepSeek-R1-Distill-Llama** variants (8B and 70B) - Knowledge distilled into Llama architecture\n",
    "- **DeepSeek-R1-Distill-Qwen** variants (1.5B to 32B) - Knowledge distilled into Qwen architecture\n",
    "\n",
    "This selection allows us to analyze:\n",
    "1. How model size affects clinical summarization quality\n",
    "2. Whether the base architecture (Llama vs Qwen) impacts performance\n",
    "3. What performance tradeoffs come with using smaller distilled models\n",
    "\n",
    "The smaller distilled models could be particularly valuable in resource-constrained clinical settings if they maintain adequate performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying Models for the DeepSeek Evaluation\n",
    "\n",
    "To fully execute this notebook, you'll need to deploy the DeepSeek models yourself so that Lumigator can access them:\n",
    "\n",
    "1. **Set up model deployments** for the DeepSeek models (both Llama and Qwen variants)\n",
    "2. **Configure your `.env` file** with the IP addresses of your deployed models:\n",
    "   ```\n",
    "   # Llama models\n",
    "   LLAMA_8B_IP=<your-deployment-ip>\n",
    "   LLAMA_70B_IP=<your-deployment-ip>\n",
    "   \n",
    "   # Qwen models\n",
    "   QWEN_1_5B_IP=<your-deployment-ip>\n",
    "   QWEN_7B_IP=<your-deployment-ip>\n",
    "   QWEN_14B_IP=<your-deployment-ip>\n",
    "   QWEN_32B_IP=<your-deployment-ip>\n",
    "   ```\n",
    "\n",
    "For detailed instructions on how to deploy DeepSeek models on Kubernetes, see the guide on the Mozilla.ai blog: [Deploying DeepSeek V3 on Kubernetes](https://blog.mozilla.ai/deploying-deepseek-v3-on-kubernetes/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the models we want to evaluate\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from utils import create_deepseek_config\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "evaluations = [\n",
    "    # Note that you need to have run Lumigator with the DEEPSEEK_API_KEY environment variable set,\n",
    "    # so that the Lumigator server can access the DeepSeek API\n",
    "    {\n",
    "        \"name\": \"DeepSeek R1\",\n",
    "        \"description\": \"DeepSeek R1 https://api-docs.deepseek.com/quick_start/pricing\",\n",
    "        \"model\": \"deepseek-reasoner\",\n",
    "        \"provider\": \"deepseek\",\n",
    "    },\n",
    "    # vLLM deployments - Llama models\n",
    "    create_deepseek_config(model_name=\"DeepSeek-R1-Distill-Llama-8B\", ip_address=os.getenv(\"LLAMA_8B_IP\")),\n",
    "    create_deepseek_config(model_name=\"DeepSeek-R1-Distill-Llama-70B\", ip_address=os.getenv(\"LLAMA_70B_IP\")),\n",
    "    # vLLM deployments - Qwen models\n",
    "    create_deepseek_config(model_name=\"DeepSeek-R1-Distill-Qwen-1.5B\", ip_address=os.getenv(\"QWEN_1_5B_IP\")),\n",
    "    create_deepseek_config(model_name=\"DeepSeek-R1-Distill-Qwen-7B\", ip_address=os.getenv(\"QWEN_7B_IP\")),\n",
    "    create_deepseek_config(model_name=\"DeepSeek-R1-Distill-Qwen-14B\", ip_address=os.getenv(\"QWEN_14B_IP\")),\n",
    "    create_deepseek_config(model_name=\"DeepSeek-R1-Distill-Qwen-32B\", ip_address=os.getenv(\"QWEN_32B_IP\")),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance of the Custom System Prompt for Clinical Summarization\n",
    "\n",
    "The custom system prompt is critical to the clinical conversation summarization task for several reasons:\n",
    "\n",
    "1. **Domain-specific guidance**: By specifying that the model should act as an \"expert medical scribe,\" we establish the specialized knowledge domain and expected level of expertise.\n",
    "\n",
    "2. **Task definition**: The prompt clearly defines the task of converting conversational medical dialogue into a structured Assessment & Plan (A&P) document, which requires significant information distillation and reorganization.\n",
    "\n",
    "3. **Format standardization**: The instruction to create \"problem oriented\" summaries in \"narrative paragraph form\" ensures consistent outputs across all model evaluations, making comparisons more meaningful.\n",
    "\n",
    "4. **Clinical comprehensiveness**: By explicitly requesting information about \"medical treatment, patient consent, patient education and counseling, and medical reasoning,\" the prompt ensures the models capture all critical components of medical documentation.\n",
    "\n",
    "5. **Zero-shot performance**: Without this prompt, models would lack the context necessary to produce clinically useful summaries, especially the smaller distilled models being evaluated.\n",
    "\n",
    "6. **Bias reduction**: The consistent prompt reduces variability in how different models interpret the task, allowing for more direct comparison of their inherent capabilities in medical summarization.\n",
    "\n",
    "This prompt essentially serves as a controlled variable in our experiment, allowing us to focus on how different DeepSeek model variants perform on the same well-defined clinical task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an expert medical scribe who is tasked with reading the transcript of a conversation between a doctor and a patient\n",
    "and generating a concise and comprehensive Assessment & Plan (A&P) summary.\n",
    "Please follow the best standards and practices for modern scribe documentation.\n",
    "The A&P should be problem oriented, but write in a narrative paragraph form, without any fancy formatting.\n",
    "When appropriate, please include information about medical treatment, patient consent, patient education and counseling, and medical reasoning.\n",
    "\"\"\".strip()  # noqa: E501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lumigator_sdk.strict_schemas import WorkflowCreateRequest\n",
    "\n",
    "# Configure generation parameters to ensure deterministic, high-quality outputs\n",
    "# - temperature=0.0: Makes output deterministic (no randomness)\n",
    "# - top_p=0.9: Limits token selection to the most probable ones\n",
    "# - max_new_tokens=512: Caps response length appropriately for clinical summaries\n",
    "# - frequency_penalty=0.0: No penalty for token repetition\n",
    "generation_config = {\n",
    "    \"temperature\": 0.0,\n",
    "    \"top_p\": 0.9,\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"frequency_penalty\": 0.0,\n",
    "}\n",
    "\n",
    "# Create a workflow for each model configuration in our evaluation list\n",
    "# Each workflow represents a single model's inference evaluation against the dataset\n",
    "# within the experiment, allowing for systematic comparison of results\n",
    "for evaluation_config in evaluations:\n",
    "    request = WorkflowCreateRequest(\n",
    "        name=evaluation_config[\"name\"],\n",
    "        description=evaluation_config[\"description\"],\n",
    "        model=evaluation_config[\"model\"],\n",
    "        provider=evaluation_config[\"provider\"],\n",
    "        base_url=evaluation_config.get(\"base_url\"),\n",
    "        dataset=dataset_id,\n",
    "        experiment_id=experiment_id,\n",
    "        system_prompt=system_prompt,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "    client.workflows.create_workflow(request).model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing the Evaluation Workflows\n",
    "\n",
    "With all workflows now created, Lumigator will:\n",
    "\n",
    "1. Generate summaries from each model for every example in the dataset\n",
    "2. Calculate performance metrics like ROUGE, BLEU, and BERTScore\n",
    "3. Make all results available for comparison\n",
    "\n",
    "This automated evaluation approach ensures consistent testing conditions across all models. The wait_for_all_workflows function will poll the Lumigator API until all workflows complete, allowing us to retrieve and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from lumigator_schemas.workflows import WorkflowStatus\n",
    "from utils import wait_for_all_workflows\n",
    "\n",
    "experiment = wait_for_all_workflows(client, 1)\n",
    "print(f\"Experiment: {experiment.name}\")\n",
    "for workflow in experiment.workflows:\n",
    "    print(f\"--------{workflow.name}--------\")\n",
    "    print(f\"Desc: {workflow.description}\")\n",
    "    print(json.dumps(workflow.metrics, indent=2))\n",
    "    if workflow.status == WorkflowStatus.SUCCEEDED:\n",
    "        response = requests.get(workflow.artifacts_download_url)\n",
    "        result = response.json()\n",
    "        # print the first prediction\n",
    "        print(result[0][\"prediction\"])\n",
    "    # else:\n",
    "    #     print(f\"Workflow {workflow.id} failed: deleting the workflow.\")\n",
    "    #     client.workflows.delete_workflow(workflow.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b56a6_row0_col0, #T_b56a6_row0_col2, #T_b56a6_row0_col3, #T_b56a6_row0_col5, #T_b56a6_row1_col3, #T_b56a6_row3_col1, #T_b56a6_row3_col4 {\n",
       "  background-color: #08306b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b56a6_row0_col1 {\n",
       "  background-color: #2575b7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b56a6_row0_col4, #T_b56a6_row2_col0 {\n",
       "  background-color: #084387;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b56a6_row1_col0, #T_b56a6_row1_col2, #T_b56a6_row3_col3, #T_b56a6_row4_col1, #T_b56a6_row4_col4, #T_b56a6_row4_col5 {\n",
       "  background-color: #f7fbff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b56a6_row1_col1 {\n",
       "  background-color: #d0e1f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b56a6_row1_col4 {\n",
       "  background-color: #dbe9f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b56a6_row1_col5 {\n",
       "  background-color: #bad6eb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b56a6_row2_col1 {\n",
       "  background-color: #94c4df;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b56a6_row2_col2 {\n",
       "  background-color: #1562a9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b56a6_row2_col3 {\n",
       "  background-color: #105ba4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b56a6_row2_col4 {\n",
       "  background-color: #3484bf;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b56a6_row2_col5, #T_b56a6_row3_col5 {\n",
       "  background-color: #2b7bba;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b56a6_row3_col0 {\n",
       "  background-color: #4493c7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b56a6_row3_col2 {\n",
       "  background-color: #5fa6d1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b56a6_row4_col0 {\n",
       "  background-color: #eef5fc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b56a6_row4_col2 {\n",
       "  background-color: #c1d9ed;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b56a6_row4_col3 {\n",
       "  background-color: #d6e6f4;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b56a6\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b56a6_level0_col0\" class=\"col_heading level0 col0\" >ROUGE-1</th>\n",
       "      <th id=\"T_b56a6_level0_col1\" class=\"col_heading level0 col1\" >ROUGE-2</th>\n",
       "      <th id=\"T_b56a6_level0_col2\" class=\"col_heading level0 col2\" >ROUGE-L</th>\n",
       "      <th id=\"T_b56a6_level0_col3\" class=\"col_heading level0 col3\" >BERTScore</th>\n",
       "      <th id=\"T_b56a6_level0_col4\" class=\"col_heading level0 col4\" >METEOR</th>\n",
       "      <th id=\"T_b56a6_level0_col5\" class=\"col_heading level0 col5\" >BLEU</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b56a6_level0_row0\" class=\"row_heading level0 row0\" >DeepSeek-R1-Distill-Llama-70B</th>\n",
       "      <td id=\"T_b56a6_row0_col0\" class=\"data row0 col0\" >29.6</td>\n",
       "      <td id=\"T_b56a6_row0_col1\" class=\"data row0 col1\" >8.8</td>\n",
       "      <td id=\"T_b56a6_row0_col2\" class=\"data row0 col2\" >15.2</td>\n",
       "      <td id=\"T_b56a6_row0_col3\" class=\"data row0 col3\" >83.8</td>\n",
       "      <td id=\"T_b56a6_row0_col4\" class=\"data row0 col4\" >34.6</td>\n",
       "      <td id=\"T_b56a6_row0_col5\" class=\"data row0 col5\" >3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b56a6_level0_row1\" class=\"row_heading level0 row1\" >DeepSeek-R1-Distill-Llama-8B vLLM</th>\n",
       "      <td id=\"T_b56a6_row1_col0\" class=\"data row1 col0\" >25.4</td>\n",
       "      <td id=\"T_b56a6_row1_col1\" class=\"data row1 col1\" >8.0</td>\n",
       "      <td id=\"T_b56a6_row1_col2\" class=\"data row1 col2\" >12.6</td>\n",
       "      <td id=\"T_b56a6_row1_col3\" class=\"data row1 col3\" >83.8</td>\n",
       "      <td id=\"T_b56a6_row1_col4\" class=\"data row1 col4\" >32.4</td>\n",
       "      <td id=\"T_b56a6_row1_col5\" class=\"data row1 col5\" >2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b56a6_level0_row2\" class=\"row_heading level0 row2\" >DeepSeek-R1-Distill-Qwen-32B vLLM</th>\n",
       "      <td id=\"T_b56a6_row2_col0\" class=\"data row2 col0\" >29.3</td>\n",
       "      <td id=\"T_b56a6_row2_col1\" class=\"data row2 col1\" >8.3</td>\n",
       "      <td id=\"T_b56a6_row2_col2\" class=\"data row2 col2\" >14.7</td>\n",
       "      <td id=\"T_b56a6_row2_col3\" class=\"data row2 col3\" >83.7</td>\n",
       "      <td id=\"T_b56a6_row2_col4\" class=\"data row2 col4\" >33.9</td>\n",
       "      <td id=\"T_b56a6_row2_col5\" class=\"data row2 col5\" >3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b56a6_level0_row3\" class=\"row_heading level0 row3\" >DeepSeek-R1-Distill-Qwen-14B vLLM</th>\n",
       "      <td id=\"T_b56a6_row3_col0\" class=\"data row3 col0\" >28.0</td>\n",
       "      <td id=\"T_b56a6_row3_col1\" class=\"data row3 col1\" >9.2</td>\n",
       "      <td id=\"T_b56a6_row3_col2\" class=\"data row3 col2\" >14.0</td>\n",
       "      <td id=\"T_b56a6_row3_col3\" class=\"data row3 col3\" >83.2</td>\n",
       "      <td id=\"T_b56a6_row3_col4\" class=\"data row3 col4\" >34.8</td>\n",
       "      <td id=\"T_b56a6_row3_col5\" class=\"data row3 col5\" >3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b56a6_level0_row4\" class=\"row_heading level0 row4\" >DeepSeek-R1-Distill-Qwen-7B vLLM</th>\n",
       "      <td id=\"T_b56a6_row4_col0\" class=\"data row4 col0\" >25.6</td>\n",
       "      <td id=\"T_b56a6_row4_col1\" class=\"data row4 col1\" >7.7</td>\n",
       "      <td id=\"T_b56a6_row4_col2\" class=\"data row4 col2\" >13.3</td>\n",
       "      <td id=\"T_b56a6_row4_col3\" class=\"data row4 col3\" >83.3</td>\n",
       "      <td id=\"T_b56a6_row4_col4\" class=\"data row4 col4\" >32.0</td>\n",
       "      <td id=\"T_b56a6_row4_col5\" class=\"data row4 col5\" >2.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1202fc110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# First, let's deduplicate and organize the results\n",
    "model_results = defaultdict(dict)\n",
    "unique_models = set()\n",
    "\n",
    "# Process results and remove duplicates\n",
    "for workflow in experiment.workflows:\n",
    "    model_name = workflow.name\n",
    "    unique_models.add(model_name)\n",
    "    for metric, value in workflow.metrics.items():\n",
    "        model_results[model_name][metric] = value * 100\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "results_df = pd.DataFrame.from_dict(model_results, orient=\"index\")\n",
    "\n",
    "\n",
    "# Sort for readability - order by model architecture and size\n",
    "def extract_size(model_name):\n",
    "    if \"70B\" in model_name:\n",
    "        return 70\n",
    "    elif \"32B\" in model_name:\n",
    "        return 32\n",
    "    elif \"14B\" in model_name:\n",
    "        return 14\n",
    "    elif \"8B\" in model_name:\n",
    "        return 8\n",
    "    elif \"7B\" in model_name:\n",
    "        return 7\n",
    "    elif \"1.5B\" in model_name:\n",
    "        return 1.5\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def extract_arch(model_name):\n",
    "    if \"Llama\" in model_name:\n",
    "        return \"Llama\"\n",
    "    elif \"Qwen\" in model_name:\n",
    "        return \"Qwen\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "\n",
    "# Add columns for sorting\n",
    "results_df[\"size\"] = results_df.index.map(extract_size)\n",
    "results_df[\"architecture\"] = results_df.index.map(extract_arch)\n",
    "\n",
    "# Sort by architecture and then by descending size\n",
    "results_df = results_df.sort_values(by=[\"architecture\", \"size\"], ascending=[True, False])\n",
    "\n",
    "# Select just the most relevant metrics for display\n",
    "display_metrics = [\"rouge1_mean\", \"rouge2_mean\", \"rougeL_mean\", \"bertscore_f1_mean\", \"meteor_mean\", \"bleu_mean\"]\n",
    "display_df = results_df[display_metrics].copy()\n",
    "\n",
    "# Rename columns for readability\n",
    "display_df.columns = [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"BERTScore\", \"METEOR\", \"BLEU\"]\n",
    "\n",
    "# Display as formatted table\n",
    "styled_df = display_df.style.format(\"{:.1f}\").background_gradient(cmap=\"Blues\")\n",
    "display(styled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Conclusions\n",
    "\n",
    "The metrics above provide quantitative measures of how well each model performed on the clinical summarization task. Key performance indicators include:\n",
    "\n",
    "- **ROUGE scores** - Measure of overlap between generated and reference summaries\n",
    "- **BERTScore** - Semantic similarity between generated and reference text\n",
    "- **Processing time** - Indicates inference speed differences between models\n",
    "\n",
    "Looking at these results, we can draw insights about:\n",
    "\n",
    "1. The performance-to-size tradeoff for different DeepSeek distilled models\n",
    "2. Whether smaller models maintain sufficient quality for practical clinical use\n",
    "3. How the base architecture influences summarization capabilities\n",
    "\n",
    "This evaluation demonstrates Lumigator's ability to facilitate structured comparisons between language models on specialized tasks like clinical documentation.\n",
    "\n",
    "For production applications, additional considerations beyond these metrics would include:\n",
    "- Factual accuracy of medical content\n",
    "- Adherence to clinical documentation standards\n",
    "- Robustness across different medical specialties"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
