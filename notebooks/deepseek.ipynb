{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Deepseek/Mistral/OpenAI Models using Lumigator 🐊\n",
    "\n",
    "There's been a lot of hype around [Deepseek R-1](https://github.com/deepseek-ai/DeepSeek-R1): \n",
    "it's an open source model that rivals OpenAIs o1 performance!\n",
    "\n",
    "In this notebook, we will use Lumigator in order to evaluate Deepseek R-1 against OpenAI o1 and Mistral Large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Neither GPT-4o or DeepSeek R-1 have published exactly what data they've used for training. This means\n",
    "that anything on the internet may have been used in its training and we have no way to verify that \n",
    "any public data wasn't used in the training process. This makes any evaluation we do with public data\n",
    "inherently flawed. If we post it on the internet, it's technically possible to use for LLM training and\n",
    "is no longer a reliable benchmark for future models. \n",
    "\n",
    "That's a big caveat to this notebook demonstration: the model performance differences don't actually \n",
    "indicate which model is better, in order to answer that question you'll have to try it on your own data\n",
    "that couldn't possibly have been used for training DeepSeek R-1 or GPT-4o!\n",
    "\n",
    "With that in mind, the dataset we'll use here is called [SummScreen](https://arxiv.org/abs/2104.07091) ForeverDreaming.\n",
    "It's a dataset of tv show transcripts and their associated recaps, and for this demo we'll filter down to using only episodes\n",
    "from the popular US tv show called \"The Office\". This filtered dataset is useful for a few reasons:\n",
    "\n",
    "1. The input transcripts are quite long (>4k tokens) which means that generating a summary isn't trivial task.\n",
    "2. I'm a domain expert in this because I've watched all of the episodes of The Office many times.\n",
    " I'll be able to evaluate for myself how good the summary is, and I'll know if a model missed anything in its summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Requisites\n",
    "\n",
    "Before running this notebook, you need to have Lumigator running.You will need to ensure that you have both `OPENAI_API_KEY`, `DEEPSEEK_API_KEY`, and `MISTRAL_API_KEY` set in your environment variables. Then, run `make local-up` in order to build and have Lumigator listening. Now Lumigator should be ready for our experiment and be able to make requests to OpenAI, Mistral, and DeepSeek!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The filtered test split contains 14 examples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f570447d1bb4777b39ea294ea2f6716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# First step, let's prepare the dataset!\n",
    "\n",
    "# First, grab the dataset off huggingface: https://huggingface.co/datasets/YuanPJ/summ_screen\n",
    "ds = load_dataset(\"YuanPJ/summ_screen\", \"fd\")[\"test\"]\n",
    "# filter for only examples which contain \"Gilmore_Girls\" in the File Name\n",
    "ds = ds.filter(lambda x: \"The_Office\" in x[\"File Name\"])\n",
    "\n",
    "# Now let's prepare it for Lumigator upload. We need to rename some columns and delete the rest\n",
    "# rename the column \"input\" to \"text\" and \"output\" to \"ground_truth\". This is what Lumigator expects\n",
    "ds = ds.rename_column(\"Transcript\", \"examples\")\n",
    "ds = ds.rename_column(\"Recap\", \"ground_truth\")\n",
    "\n",
    "# remove all columns except \"text\" and \"ground_truth\"\n",
    "columns_list = ds.column_names\n",
    "columns_list.remove(\"examples\")\n",
    "columns_list.remove(\"ground_truth\")\n",
    "ds = ds.remove_columns(columns_list)\n",
    "\n",
    "print(f\"The filtered test split contains {len(ds)} examples.\")\n",
    "# convert ds to a csv and make it a string so we can upload it with the Lumigator API\n",
    "DS_OUTPUT = \"office_dataset.csv\"\n",
    "ds.to_csv(DS_OUTPUT)\n",
    "MAX_SAMPLES = 1  # This demo is only designed to run on example, to make visual comparison easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset uploaded and has ID: 68081385-4a8d-46ad-99f0-1c53da0373de\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from time import sleep\n",
    "\n",
    "from lumigator_schemas.datasets import DatasetFormat\n",
    "from lumigator_schemas.experiments import GetExperimentResponse\n",
    "from lumigator_schemas.workflows import WorkflowCreateRequest, WorkflowStatus\n",
    "from lumigator_sdk.lumigator import LumigatorClient\n",
    "from lumigator_sdk.strict_schemas import ExperimentIdCreate\n",
    "\n",
    "\n",
    "def wait_for_all_workflows(\n",
    "    lumi_client_int: LumigatorClient, experiment_id: str\n",
    ") -> GetExperimentResponse:\n",
    "    \"\"\"Wait for an experiment to complete.\"\"\"\n",
    "    still_running = True\n",
    "    while still_running:\n",
    "        still_running = False\n",
    "        experiment_details = lumi_client_int.experiments.get_experiment(experiment_id)\n",
    "        still_running_workflows = []\n",
    "        for workflow in experiment_details.workflows:\n",
    "            if workflow.status not in [WorkflowStatus.SUCCEEDED, WorkflowStatus.FAILED]:\n",
    "                still_running_workflows.append(workflow.name)\n",
    "        if still_running_workflows:\n",
    "            still_running = True\n",
    "            print(f\"Waiting for workflows {still_running_workflows} to complete\")\n",
    "            sleep(10)\n",
    "    return experiment_details\n",
    "\n",
    "\n",
    "# Time to connect up to the Lumigator client!\n",
    "LUMI_HOST = \"localhost:8000\"\n",
    "client = LumigatorClient(api_host=LUMI_HOST)\n",
    "\n",
    "# Upload that file that we created earlier\n",
    "with Path.open(DS_OUTPUT) as file:\n",
    "    data = file.read()\n",
    "dataset_response = client.datasets.create_dataset(dataset=data, format=DatasetFormat.JOB)\n",
    "dataset_id = dataset_response.id\n",
    "print(f\"Dataset uploaded and has ID: {dataset_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment created and has ID: 365350851358729462\n"
     ]
    }
   ],
   "source": [
    "# Now time to create an experiment in Lumigator! This is a container for all the workflows we'll run\n",
    "request = ExperimentIdCreate(\n",
    "    name=\"The Office Summarization\",\n",
    "    description=\"Bears, Beets, Battlestar Galactica\",\n",
    ")\n",
    "experiment_response = client.experiments.create_experiment(request)\n",
    "experiment_id = experiment_response.id\n",
    "print(f\"Experiment created and has ID: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lumigator_schemas.workflows import WorkflowDetailsResponse\n",
    "\n",
    "\n",
    "# Wait till the workflow is done\n",
    "def get_workflow_results(workflow: WorkflowDetailsResponse):\n",
    "    response = requests.get(workflow.artifacts_download_url)\n",
    "    result = response.json()\n",
    "\n",
    "    results = {\n",
    "        \"rouge2\": round(result[\"rouge\"][\"rouge2_mean\"], 2),\n",
    "        \"bertscore\": round(result[\"bertscore\"][\"f1_mean\"], 2),\n",
    "        \"meteor\": round(result[\"meteor\"][\"meteor_mean\"], 2),\n",
    "        \"predictions\": result[\"predictions\"],\n",
    "        \"ground_truth\": result[\"ground_truth\"],\n",
    "        \"examples\": result[\"examples\"],\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '646c43ca25d141c5880c0233993c8865',\n",
       " 'experiment_id': '365350851358729462',\n",
       " 'name': 'Deepseek R1',\n",
       " 'description': 'Summarize with Deepseek R-1',\n",
       " 'status': <WorkflowStatus.CREATED: 'created'>,\n",
       " 'created_at': datetime.datetime(2025, 2, 7, 15, 28, 54, 250000),\n",
       " 'updated_at': None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's run the Deepseek R1 https://api-docs.deepseek.com/quick_start/pricing\n",
    "request = WorkflowCreateRequest(\n",
    "    name=\"Deepseek R1\",\n",
    "    description=\"Summarize with Deepseek R-1\",\n",
    "    model=\"deepseek/deepseek-reasoner\",\n",
    "    model_url=\"deepseek/deepseek-reasoner\",\n",
    "    dataset=dataset_id,\n",
    "    experiment_id=experiment_id,\n",
    "    max_samples=MAX_SAMPLES,\n",
    ")\n",
    "client.workflows.create_workflow(request).model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'a61069892211418db67fd75ce20ed7c0',\n",
       " 'experiment_id': '365350851358729462',\n",
       " 'name': 'Deepseek V3',\n",
       " 'description': 'Summarize with Deepseek V3',\n",
       " 'status': <WorkflowStatus.CREATED: 'created'>,\n",
       " 'created_at': datetime.datetime(2025, 2, 7, 15, 28, 54, 944000),\n",
       " 'updated_at': None}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's run the Deepseek V3 https://api-docs.deepseek.com/quick_start/pricing\n",
    "request = WorkflowCreateRequest(\n",
    "    name=\"Deepseek V3\",\n",
    "    description=\"Summarize with Deepseek V3\",\n",
    "    model=\"deepseek/deepseek-chat\",\n",
    "    model_url=\"deepseek/deepseek-chat\",\n",
    "    dataset=dataset_id,\n",
    "    experiment_id=experiment_id,\n",
    "    max_samples=MAX_SAMPLES,\n",
    ")\n",
    "client.workflows.create_workflow(request).model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '88947246f79445449a73fd98bd6e146e',\n",
       " 'experiment_id': '365350851358729462',\n",
       " 'name': 'Mistral Large',\n",
       " 'description': 'Summarize with Mistral Lage',\n",
       " 'status': <WorkflowStatus.CREATED: 'created'>,\n",
       " 'created_at': datetime.datetime(2025, 2, 7, 15, 28, 55, 366000),\n",
       " 'updated_at': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's run the Deepseek V3 https://api-docs.deepseek.com/quick_start/pricing\n",
    "request = WorkflowCreateRequest(\n",
    "    name=\"Mistral Large\",\n",
    "    description=\"Summarize with Mistral Lage\",\n",
    "    model=\"mistral/mistral-large-latest\",\n",
    "    model_url=\"mistral/mistral-large-latest\",\n",
    "    dataset=dataset_id,\n",
    "    experiment_id=experiment_id,\n",
    "    max_samples=MAX_SAMPLES,\n",
    ")\n",
    "client.workflows.create_workflow(request).model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '4b8ba9658a12434dae8f812ba759bf6e',\n",
       " 'experiment_id': '365350851358729462',\n",
       " 'name': 'OpenAI o1',\n",
       " 'description': 'Summarize with o1',\n",
       " 'status': <WorkflowStatus.CREATED: 'created'>,\n",
       " 'created_at': datetime.datetime(2025, 2, 7, 15, 28, 55, 451000),\n",
       " 'updated_at': None}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's run the same thing, but with o3-mini\n",
    "request = WorkflowCreateRequest(\n",
    "    name=\"OpenAI o1\",\n",
    "    description=\"Summarize with o1\",\n",
    "    model=\"o1\",\n",
    "    model_url=\"o1\",\n",
    "    dataset=dataset_id,\n",
    "    experiment_id=experiment_id,\n",
    "    max_samples=MAX_SAMPLES,\n",
    ")\n",
    "client.workflows.create_workflow(request).model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '81e36d1d3df04e6a9d930e1281a59257',\n",
       " 'experiment_id': '365350851358729462',\n",
       " 'name': 'OpenAI o3-mini',\n",
       " 'description': 'Summarize with o3-mini',\n",
       " 'status': <WorkflowStatus.CREATED: 'created'>,\n",
       " 'created_at': datetime.datetime(2025, 2, 7, 15, 28, 55, 558000),\n",
       " 'updated_at': None}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's run the same thing, but with GPT-40\n",
    "request = WorkflowCreateRequest(\n",
    "    name=\"OpenAI o3-mini\",\n",
    "    description=\"Summarize with o3-mini\",\n",
    "    model=\"o3-mini\",\n",
    "    model_url=\"o3-mini\",\n",
    "    dataset=dataset_id,\n",
    "    experiment_id=experiment_id,\n",
    "    max_samples=MAX_SAMPLES,\n",
    ")\n",
    "client.workflows.create_workflow(request).model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for workflows ['OpenAI o3-mini', 'OpenAI o1', 'Mistral Large', 'Deepseek V3', 'Deepseek R1'] to complete\n",
      "Waiting for workflows ['OpenAI o3-mini', 'OpenAI o1', 'Mistral Large', 'Deepseek V3', 'Deepseek R1'] to complete\n",
      "Waiting for workflows ['OpenAI o3-mini', 'OpenAI o1', 'Mistral Large', 'Deepseek V3', 'Deepseek R1'] to complete\n",
      "Waiting for workflows ['OpenAI o3-mini', 'OpenAI o1', 'Mistral Large', 'Deepseek V3', 'Deepseek R1'] to complete\n",
      "Waiting for workflows ['OpenAI o3-mini', 'OpenAI o1', 'Mistral Large', 'Deepseek V3', 'Deepseek R1'] to complete\n",
      "Waiting for workflows ['OpenAI o3-mini', 'OpenAI o1', 'Mistral Large', 'Deepseek V3', 'Deepseek R1'] to complete\n",
      "Waiting for workflows ['OpenAI o3-mini', 'OpenAI o1', 'Mistral Large', 'Deepseek V3', 'Deepseek R1'] to complete\n",
      "Waiting for workflows ['OpenAI o3-mini', 'OpenAI o1', 'Mistral Large', 'Deepseek V3', 'Deepseek R1'] to complete\n",
      "Waiting for workflows ['OpenAI o3-mini', 'OpenAI o1', 'Mistral Large', 'Deepseek V3', 'Deepseek R1'] to complete\n",
      "Waiting for workflows ['OpenAI o3-mini', 'OpenAI o1', 'Mistral Large', 'Deepseek V3', 'Deepseek R1'] to complete\n",
      "Waiting for workflows ['OpenAI o3-mini', 'OpenAI o1', 'Deepseek V3', 'Deepseek R1'] to complete\n",
      "Waiting for workflows ['OpenAI o3-mini', 'OpenAI o1', 'Deepseek V3'] to complete\n",
      "Experiment: The Office Summarization\n",
      "--------OpenAI o3-mini--------\n",
      "Desc: Summarize with o3-mini\n",
      "ROUGE2: 0.05\n",
      "BERTScore: 0.8\n",
      "METEOR: 0.24\n",
      "Example 0\n",
      "Prediction: \n",
      "This excerpt is a long, dialogue-heavy scene from a script (likely from The Office) where Michael Scott organizes a friendly basketball game between the Dunder Mifflin office staff and the warehouse crew. Michael enthusiastically explains his idea, mixing his typical mix of misguided leadership, humor, and insensitivity. He assigns roles for the game, makes awkward attempts to motivate and tease the team (often offending or embarrassing staff like Dwight, Pam, and Phyllis), and juggles other workplace issues like scheduling weekend work. As the game unfolds, the characters exhibit their typical quirks—from Michael’s over-the-top coaching and inappropriate banter, to Dwight’s aggressive competitiveness and Jim’s bemused commentary. Amid playful trash talk, physical mishaps, and the evolving game dynamics, Michael’s leadership is both the catalyst and comic relief. Ultimately, after a series of misadventures and a contentious finish, Michael declares a win (with the companion threat of weekend work as a friendly penalty) but then backs off, granting the team an extra day off, emphasizing even in sports, character and teamwork matter.\n",
      "\n",
      "Completion Tokens: 232\n",
      "Prompt Tokens: 4818\n",
      "Reasoning Tokens: {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 192, 'rejected_prediction_tokens': 0}\n",
      "--------OpenAI o1--------\n",
      "Desc: Summarize with o1\n",
      "ROUGE2: 0.08\n",
      "BERTScore: 0.8\n",
      "METEOR: 0.32\n",
      "Example 0\n",
      "Prediction: \n",
      "In this sequence, Michael arranges a lunchtime basketball game between the office staff and the warehouse crew. He volunteers several employees for the office team—reluctantly including Dwight—and makes an offhand bet that the losers must come in on Saturday.\n",
      "\n",
      "As they prepare, Dwight pesters Michael to be team captain or manager, which Michael refuses. Tension arises when Dwight tries to assign Saturday shifts and Jim pushes back, poking fun at Dwight’s newfound “power.”\n",
      "\n",
      "On the court, Michael struggles, but Jim reveals unexpected skill and ensures the office team stays competitive. Roy, Pam’s fiancé, also plays for the warehouse and squares off against Jim. Things get heated when Michael calls fouls and abruptly ends the game, declaring the office winners. Initially, he tries to force the warehouse workers to come in on Saturday, but then backtracks, telling everyone to take the day off. Throughout, Jim and Pam share friendly banter hinting at their closeness despite her engagement to Roy.\n",
      "\n",
      "Completion Tokens: 208\n",
      "Prompt Tokens: 4818\n",
      "Reasoning Tokens: {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 320, 'rejected_prediction_tokens': 0}\n",
      "--------Mistral Large--------\n",
      "Desc: Summarize with Mistral Lage\n",
      "Workflow 88947246f79445449a73fd98bd6e146e failed: deleting the workflow.\n",
      "--------Deepseek V3--------\n",
      "Desc: Summarize with Deepseek V3\n",
      "ROUGE2: 0.08\n",
      "BERTScore: 0.81\n",
      "METEOR: 0.29\n",
      "Example 0\n",
      "Prediction: \n",
      "In this series of scenes from *The Office*, Michael Scott organizes a friendly basketball game between the office staff and the warehouse workers. Michael is overly confident in his basketball skills and sees the game as a team-building exercise. He assembles a team, excluding Dwight due to past behavior, but assigns him to manage weekend work schedules. Dwight later tries to assert his authority by assigning Jim to work on Saturday, leading to some office tension.\n",
      "\n",
      "The game itself is chaotic, with Michael making exaggerated claims about his skills and struggling during the match. The warehouse workers, led by Roy and Darryl, prove to be strong competitors. Jim steps up and plays well, even engaging in some competitive banter with Roy, who is Pam's fiancé. The game ends abruptly when Michael claims a foul and declares the office team the winners, though it’s unclear who actually won. Michael jokes about making the warehouse staff work on Saturday but later reveals it was a prank, giving everyone the day off.\n",
      "\n",
      "Throughout the episode, Michael’s over-the-top enthusiasm and lack of self-awareness are on full display, while Jim and Pam share subtle moments of connection, hinting at their underlying chemistry. The game serves as a backdrop for office dynamics, competition, and Michael’s attempts to boost morale, even if his methods are often misguided.\n",
      "\n",
      "Completion Tokens: 266\n",
      "Prompt Tokens: 5133\n",
      "Reasoning Tokens: None\n",
      "--------Deepseek R1--------\n",
      "Desc: Summarize with Deepseek R-1\n",
      "Workflow 646c43ca25d141c5880c0233993c8865 failed: deleting the workflow.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "experiment = wait_for_all_workflows(client, experiment_id)\n",
    "print(f\"Experiment: {experiment.name}\")\n",
    "# create a table with the results\n",
    "table = pd.DataFrame()\n",
    "for workflow in experiment.workflows:\n",
    "    print(f\"--------{workflow.name}--------\")\n",
    "    print(f\"Desc: {workflow.description}\")\n",
    "    if workflow.status == WorkflowStatus.SUCCEEDED:\n",
    "        results = get_workflow_results(workflow)\n",
    "        print(f\"ROUGE2: {results['rouge2']}\")\n",
    "        print(f\"BERTScore: {results['bertscore']}\")\n",
    "        print(f\"METEOR: {results['meteor']}\")\n",
    "        for idx, prediction in enumerate(results[\"predictions\"]):\n",
    "            hypo = prediction[\"choices\"][0][\"message\"][\"content\"]\n",
    "            comp_tok = prediction[\"usage\"][\"completion_tokens\"]\n",
    "            prompt_tok = prediction[\"usage\"][\"prompt_tokens\"]\n",
    "            reasoning_tok = prediction[\"usage\"][\"completion_tokens_details\"]\n",
    "            if reasoning_tok:\n",
    "                comp_tok = comp_tok - reasoning_tok[\"reasoning_tokens\"]\n",
    "            print(f\"Example {idx}\")\n",
    "            print(f\"Prediction: \\n{hypo}\\n\")\n",
    "            print(f\"Completion Tokens: {comp_tok}\")\n",
    "            print(f\"Prompt Tokens: {prompt_tok}\")\n",
    "            print(f\"Reasoning Tokens: {reasoning_tok}\")\n",
    "            # add a new row to the table\n",
    "            table = pd.concat(\n",
    "                [\n",
    "                    table,\n",
    "                    pd.DataFrame(\n",
    "                        {\n",
    "                            \"Model\": workflow.name,\n",
    "                            \"ROUGE2\": results[\"rouge2\"],\n",
    "                            \"BERTScore\": results[\"bertscore\"],\n",
    "                            \"METEOR\": results[\"meteor\"],\n",
    "                            \"Example\": idx,\n",
    "                            \"Prediction\": hypo,\n",
    "                            \"Tokens\": comp_tok,\n",
    "                            \"Prompt Tokens\": prompt_tok,\n",
    "                            \"Reasoning Tokens\": reasoning_tok,\n",
    "                        },\n",
    "                        index=[0],\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "    else:\n",
    "        print(f\"Workflow {workflow.id} failed: deleting the workflow.\")\n",
    "        client.workflows.delete_workflow(workflow.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Model  ROUGE2  BERTScore  METEOR  Tokens\n",
      "OpenAI o3-mini    0.05       0.80    0.24     232\n",
      "     OpenAI o1    0.08       0.80    0.32     208\n",
      "   Deepseek V3    0.08       0.81    0.29     266\n"
     ]
    }
   ],
   "source": [
    "# Generate a table the prints out all the automatic metrics and numbers for easy comparison\n",
    "print(table[[\"Model\", \"ROUGE2\", \"BERTScore\", \"METEOR\", \"Tokens\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Although we can't make any confident conclusions about which model is better overall (because all models may have had data about The Office in their training mix), DeepSeek models look to be competitive with OpenAI's and Mistral's for this summarization task. What other summarization tasks might be interesting to compare using Lumigator?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
