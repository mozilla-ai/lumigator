{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Evaluation of DeepSeek R1 Models for Clinical Conversation Summarization\n",
    "\n",
    "## Background and Motivation\n",
    "\n",
    "The DeepSeek family of models represents an interesting advancement in reasoning-specialized language models. While DeepSeek published evaluation results in [their paper](https://arxiv.org/pdf/2501.12948) (see Table 5), I wanted to understand specifically how the various Distilled R1 models compare to the full R1 model on a practical use case: clinical conversation summarization using the ACI-Bench dataset.\n",
    "\n",
    "This notebook demonstrates how to use **Lumigator** to systematically evaluate and compare these models. Lumigator provides a framework to:\n",
    "\n",
    "1. Coordinate multiple model evaluations against the same dataset\n",
    "2. Execute inference requests across different model deployments\n",
    "3. Calculate standardized metrics for performance comparison\n",
    "4. Organize and visualize the results for analysis\n",
    "\n",
    "## Getting Started with Lumigator\n",
    "\n",
    "To use this notebook, you'll need to have Lumigator running. In a terminal, run:\n",
    "\n",
    "```bash\n",
    "git clone git@github.com:mozilla-ai/lumigator.git\n",
    "cd lumigator\n",
    "make setup\n",
    "echo $DEEPSEEK_API_KEY # This shouldn't be empty\n",
    "echo $OPENAI_API_KEY # This shouldn't be empty, you need it for G-Eval metric\n",
    "make start-lumigator-build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection is: OK\n"
     ]
    }
   ],
   "source": [
    "from lumigator_sdk.lumigator import LumigatorClient\n",
    "\n",
    "# Time to connect up to the Lumigator client!\n",
    "LUMI_HOST = \"localhost:8000\"\n",
    "client = LumigatorClient(api_host=LUMI_HOST)\n",
    "print(f\"Connection is: {client.health.healthcheck().status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: ACI-Bench for Clinical Documentation\n",
    "\n",
    "This evaluation uses the ACI-Bench dataset, which was introduced in the paper \n",
    "[\"ACI-Bench: a Novel Benchmark for Ambient Clinical Intelligence\"](https://www.nature.com/articles/s41597-023-02487-3) \n",
    "(Yim et al., 2023). \n",
    "\n",
    "ACI-Bench was specifically designed to evaluate AI systems on their ability to \n",
    "understand doctor-patient conversations and generate accurate clinical documentation.\n",
    "\n",
    "### About the Dataset\n",
    "\n",
    "The test split of ACI-Bench that we'll be using consists of 40 doctor-patient conversations. \n",
    "These conversations aren't from real patient encounters but were created through professional medical simulations \n",
    "with standardized patients (actors trained to portray patients) and licensed physicians. \n",
    "\n",
    "This approach attempts to keep the data reasonably realistic while also being HIPAA-compliant, \n",
    "as no actual protected health information is included.\n",
    "\n",
    "Each conversation includes:\n",
    "\n",
    "1. A full transcript of the simulated clinical encounter, with speaker identification\n",
    "2. Human-written reference documentation\n",
    "3. Various sections of the standard clinical note format (SOAP - Subjective, Objective, Assessment, Plan)\n",
    "\n",
    "### The Assessment & Plan Task\n",
    "\n",
    "In this evaluation, we're specifically working with the **assessment and plan section** (`clef_taskC_test3_assessment_and_plan.json`), which is particularly challenging as it requires:\n",
    "\n",
    "- Identifying the patient's medical conditions\n",
    "- Understanding the physician's diagnostic reasoning\n",
    "- Summarizing the recommended treatment approach\n",
    "- Capturing follow-up plans and contingencies\n",
    "\n",
    "This section of clinical documentation represents higher-level medical reasoning compared to other sections, making it a interesting test of a model's capacity for complex medical summarization and inference.\n",
    "\n",
    "Each example in our dataset contains:\n",
    "- `examples`: The full doctor-patient conversation transcript (with speaker turns marked as `[doctor]` and `[patient]`)\n",
    "- `ground_truth`: The human-written assessment and plan section that serves as the reference summary\n",
    "- `id`: A unique identifier for each conversation\n",
    "\n",
    "The Assessment & Plan task was featured in the 2023 MEDIQA-CHAT shared task at CLEF (Conference and Labs of the Evaluation Forum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of This Evaluation\n",
    "\n",
    "This evaluation has several important limitations that should be considered when interpreting the results:\n",
    "\n",
    "1. **Unknown Training Data Exposure**: We cannot verify whether DeepSeek models were trained on the ACI-Bench dataset or similar clinical conversations. If any of these models were exposed to this data during training, they would have an unfair advantage in this evaluation - essentially having already \"seen the answers\" to the test. Without model cards or detailed training information disclosing training datasets, this remains an unknown factor.\n",
    "\n",
    "2. **Relative Comparison Focus**: Given this limitation, our analysis primarily focuses on the relative performance differences between models within the DeepSeek family, rather than making absolute claims about their capabilities for clinical summarization. By comparing models from the same family, we can still draw meaningful conclusions about how performance scales with model size and architecture (Llama vs. Qwen) when all models would have had the same potential exposure to training data.\n",
    "\n",
    "3. **Single Task Evaluation**: This evaluation examines performance on just one specific clinical documentation task (Assessment & Plan generation) and may not generalize to other medical tasks or to clinical summarization in different specialties or contexts.\n",
    "\n",
    "4. **Simulated Data**: While the ACI-Bench dataset uses realistic simulated conversations, model performance might differ on real-world clinical conversations, which tend to be messier, less structured, and potentially contain more specialized terminology.\n",
    "\n",
    "5. **Zero-Shot Setting**: Our evaluation uses a zero-shot approach with a specialized system prompt. \n",
    "Performance might improve significantly with few-shot examples.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# GitHub API URL to fetch the file list\n",
    "download_url = \"https://raw.githubusercontent.com/wyim/aci-bench/main/data/challenge_data_json/clef_taskC_test3_assessment_and_plan.json\"\n",
    "file_name = download_url.split(\"/\")[-1]\n",
    "save_dir = Path(\"data\")\n",
    "file_path = save_dir / file_name\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "response = requests.get(download_url)\n",
    "\n",
    "data = response.json()\n",
    "# convert it to a dataframe. The file by default has the columns 'src' and 'tgt'\n",
    "df = pd.DataFrame(data[\"data\"])  # noqa: PD901\n",
    "# Rename the columns to \"examples\" and \"ground_truth\", which is what the Lumigator API expects for the data\n",
    "df = df.rename(columns={\"src\": \"examples\", \"tgt\": \"ground_truth\", \"file\": \"id\"})  # noqa: PD901\n",
    "\n",
    "processed_file_path = file_path.with_suffix(\".csv\")\n",
    "# save it as a csv\n",
    "df.to_csv(processed_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now the data is all formatted: let's take a look at an example to get a feel for what the data looks like. \n",
    "Understanding the data is crucial for interpreting the results and behavior of the models being evaluated. \n",
    "\n",
    "Every dataset\n",
    "has quirks and unique things about it: in this notebook we won't dive too deeply into investigating the characteristics of the dataset,\n",
    "but it's definitely worth taking more time to understand exactly what is in a dataset before you use it for anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.iloc[0]\n",
    "print(\"--- Snippet of Conversation ---\")\n",
    "print(\"\\n\".join(sample[\"examples\"].split(\"\\n\")[6:8]))\n",
    "print(\" --- Assessment & Plan---\")\n",
    "print(sample[\"ground_truth\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Dataset into Lumigator\n",
    "Now, let's upload the dataset into lumigator using the Lumigator SDK. creating the dataset returns the dataset ID, which we will attach to future requests so that Lumigator knows which dataset should be used for running an eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset uploaded and has ID: 5e78bb11-a240-44bd-9e67-102814d5ec23\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from lumigator_schemas.datasets import DatasetFormat\n",
    "\n",
    "# Upload that file that we created earlier\n",
    "with Path.open(Path(processed_file_path), \"r\") as file:\n",
    "    data = file.read()\n",
    "dataset_response = client.datasets.create_dataset(dataset=data, format=DatasetFormat.JOB)\n",
    "dataset_id = dataset_response.id\n",
    "print(f\"Dataset uploaded and has ID: {dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Evaluation Pipeline in Lumigator\n",
    "\n",
    "Now that we've uploaded our dataset, we'll create an experiment in Lumigator. In Lumigator terminology:\n",
    "\n",
    "1. **Experiment** - A container that organizes related evaluation workflows\n",
    "2. **Workflow** - A specific model configuration being evaluated against the dataset\n",
    "3. **Dataset** - The collection of examples (in our case, clinical conversations)\n",
    "\n",
    "This structure allows us to compare multiple models on the same dataset in a systematic way, with all results organized within a single experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now time to create an experiment in Lumigator! This is a container for all the workflows we'll run\n",
    "from lumigator_schemas.experiments import ExperimentCreate\n",
    "\n",
    "experiment_id = input(\"Enter the experiment ID, or press enter to create a new experiment: \")\n",
    "if not experiment_id:\n",
    "    request = ExperimentCreate(\n",
    "        name=\"ACI-Bench clef_taskC_test3_assessment_and_plan\",\n",
    "        description=\"https://github.com/wyim/aci-bench/tree/main\",\n",
    "        dataset=dataset_id,\n",
    "    )\n",
    "    experiment_response = client.experiments.create_experiment(request)\n",
    "    experiment_id = experiment_response.id\n",
    "    print(f\"Experiment created and has ID: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection Rationale\n",
    "\n",
    "For this evaluation, we're testing a range of DeepSeek models to understand how performance scales with model size and architecture:\n",
    "\n",
    "- **DeepSeek R1** - The original reasoning-specialized model\n",
    "- **DeepSeek-R1-Distill-Llama** variants (8B and 70B) - Knowledge distilled into Llama architecture\n",
    "- **DeepSeek-R1-Distill-Qwen** variants (1.5B to 32B) - Knowledge distilled into Qwen architecture\n",
    "\n",
    "This selection allows us to analyze:\n",
    "1. How model size affects clinical summarization quality\n",
    "2. Whether the base architecture (Llama vs Qwen) impacts performance\n",
    "3. What performance tradeoffs come with using smaller distilled models\n",
    "\n",
    "The smaller distilled models could be particularly valuable in resource-constrained clinical settings if they maintain adequate performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying Models for the DeepSeek Evaluation\n",
    "\n",
    "To fully execute this notebook, you'll need to deploy the DeepSeek models yourself so that Lumigator can access them:\n",
    "\n",
    "1. **Set up model deployments** for the DeepSeek models (both Llama and Qwen variants)\n",
    "2. **Configure your `.env` file** with the IP addresses of your deployed models:\n",
    "   ```\n",
    "   # Llama models\n",
    "   LLAMA_8B_IP=<your-deployment-ip>\n",
    "   LLAMA_70B_IP=<your-deployment-ip>\n",
    "   \n",
    "   # Qwen models\n",
    "   QWEN_1_5B_IP=<your-deployment-ip>\n",
    "   QWEN_7B_IP=<your-deployment-ip>\n",
    "   QWEN_14B_IP=<your-deployment-ip>\n",
    "   QWEN_32B_IP=<your-deployment-ip>\n",
    "   ```\n",
    "\n",
    "For detailed instructions on how to deploy DeepSeek models on Kubernetes, see the guide on the Mozilla.ai blog: [Deploying DeepSeek V3 on Kubernetes](https://blog.mozilla.ai/deploying-deepseek-v3-on-kubernetes/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the models we want to evaluate\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from utils import create_evaluation_config\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "evaluations = [\n",
    "    # Note that you need to have run Lumigator with the DEEPSEEK_API_KEY environment variable set,\n",
    "    # so that the Lumigator server can access the DeepSeek API\n",
    "    {\n",
    "        \"name\": \"DeepSeek R1\",\n",
    "        \"description\": \"DeepSeek R1 https://api-docs.deepseek.com/quick_start/pricing\",\n",
    "        \"model\": \"deepseek-reasoner\",\n",
    "        \"provider\": \"deepseek\",\n",
    "    },\n",
    "    # vLLM deployments - Llama models\n",
    "    create_evaluation_config(model_name=\"DeepSeek-R1-Distill-Llama-8B\", ip_address=os.getenv(\"LLAMA_8B_IP\")),\n",
    "    create_evaluation_config(model_name=\"DeepSeek-R1-Distill-Llama-70B\", ip_address=os.getenv(\"LLAMA_70B_IP\")),\n",
    "    # # vLLM deployments - Qwen models\n",
    "    create_evaluation_config(model_name=\"DeepSeek-R1-Distill-Qwen-7B\", ip_address=os.getenv(\"QWEN_7B_IP\")),\n",
    "    create_evaluation_config(model_name=\"DeepSeek-R1-Distill-Qwen-14B\", ip_address=os.getenv(\"QWEN_14B_IP\")),\n",
    "    create_evaluation_config(model_name=\"DeepSeek-R1-Distill-Qwen-32B\", ip_address=os.getenv(\"QWEN_32B_IP\")),\n",
    "    create_evaluation_config(model_name=\"DeepSeek-R1-Distill-Qwen-1.5B\", ip_address=os.getenv(\"QWEN_1_5B_IP\")),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance of the Custom System Prompt for Clinical Summarization\n",
    "\n",
    "The custom system prompt is critical to the clinical conversation summarization task for several reasons:\n",
    "\n",
    "1. **Domain-specific guidance**: By specifying that the model should act as an \"expert medical scribe,\" we establish the specialized knowledge domain and expected level of expertise.\n",
    "\n",
    "2. **Task definition**: The prompt clearly defines the task of converting conversational medical dialogue into a structured Assessment & Plan (A&P) document, which requires significant information distillation and reorganization.\n",
    "\n",
    "3. **Format standardization**: The instruction to create \"problem oriented\" summaries in \"narrative paragraph form\" ensures consistent outputs across all model evaluations, making comparisons more meaningful.\n",
    "\n",
    "4. **Clinical comprehensiveness**: By explicitly requesting information about \"medical treatment, patient consent, patient education and counseling, and medical reasoning,\" the prompt ensures the models capture all critical components of medical documentation.\n",
    "\n",
    "5. **Zero-shot performance**: Without this prompt, models would lack the context necessary to produce clinically useful summaries, especially the smaller distilled models being evaluated.\n",
    "\n",
    "6. **Bias reduction**: The consistent prompt reduces variability in how different models interpret the task, allowing for more direct comparison of their inherent capabilities in medical summarization.\n",
    "\n",
    "This prompt essentially serves as a controlled variable in our experiment, allowing us to focus on how different DeepSeek model variants perform on the same well-defined clinical task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an expert medical scribe who is tasked with reading the transcript of a conversation between a doctor and a patient,\n",
    "and generating a concise Assessment & Plan (A&P) summary.\n",
    "Please follow the best standards and practices for modern scribe documentation.\n",
    "The A&P should be problem oriented, with the assessment being a short narrative and the plan being a list with nested bullets.\n",
    "When appropriate, please include information about medical treatment, patient consent, patient education and counseling, and medical reasoning.\n",
    "\"\"\".strip()  # noqa: E501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lumigator_sdk.strict_schemas import WorkflowCreateRequest\n",
    "\n",
    "# Configure generation parameters to ensure deterministic, high-quality outputs\n",
    "# - temperature=0.0: Makes output deterministic (no randomness)\n",
    "# - top_p=0.9: Limits token selection to the most probable ones\n",
    "# - max_new_tokens=1024: Caps response length appropriately for reasoning + clinical summaries\n",
    "# - frequency_penalty=0.0: No penalty for token repetition\n",
    "generation_config = {\n",
    "    \"temperature\": 0.0,\n",
    "    \"top_p\": 0.9,\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"frequency_penalty\": 0.0,\n",
    "}\n",
    "\n",
    "metrics: list[str] = [\"rouge\", \"g_eval_summarization\", \"token_length\"]\n",
    "\n",
    "# Create a workflow for each model configuration in our evaluation list\n",
    "# Each workflow represents a single model's inference evaluation against the dataset\n",
    "# within the experiment, allowing for systematic comparison of results\n",
    "for evaluation_config in evaluations:\n",
    "    # check if an evaluation by that name already exists\n",
    "    existing_workflows = client.experiments.get_experiment(experiment_id).workflows\n",
    "    existing_workflow = next(\n",
    "        (workflow for workflow in existing_workflows if workflow.name == evaluation_config[\"name\"]), None\n",
    "    )\n",
    "    if existing_workflow:\n",
    "        # if status is failed, delete it\n",
    "        if existing_workflow.status == \"failed\":\n",
    "            client.workflows.delete_workflow(existing_workflow.id)\n",
    "            print(f\"Deleted failed workflow {evaluation_config['name']} with ID {existing_workflow.id}\")\n",
    "        else:\n",
    "            print(f\"Workflow {evaluation_config['name']} already exists with ID {existing_workflow.id}\")\n",
    "            continue\n",
    "    request = WorkflowCreateRequest(\n",
    "        name=evaluation_config[\"name\"],\n",
    "        description=evaluation_config[\"description\"],\n",
    "        model=evaluation_config[\"model\"],\n",
    "        provider=evaluation_config[\"provider\"],\n",
    "        base_url=evaluation_config.get(\"base_url\"),\n",
    "        dataset=dataset_id,\n",
    "        experiment_id=experiment_id,\n",
    "        system_prompt=system_prompt,\n",
    "        job_timeout_sec=60 * 60 * 2,\n",
    "        generation_config=generation_config,\n",
    "        metrics=metrics,\n",
    "    )\n",
    "    created_workflow = client.workflows.create_workflow(request)\n",
    "    print(f\"Created workflow {created_workflow.name} with ID {created_workflow.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llamafile Workflows\n",
    "\n",
    "In addition to all the DeepSeek models that are running remotely in DeepSeek or our own vLLM deployment, \n",
    "let's also compare how local models run with Llamafile stack up! We'll try a few different ones, conveniently available for \n",
    "us at https://huggingface.co/collections/Bojun-Feng/deepseek-distilled-llamafiles-50b-67a471e269c04acf9aa0c79b.\n",
    "\n",
    "The amazing thing about llamafile is how simple it is! It's build on top of Llama.cpp, and using it is as simple as\n",
    "downloading the file, opening up a terminal, and running:\n",
    "\n",
    "```bash\n",
    "$ chmod +x <file_name>.llamafile\n",
    "$ ./<file_name>.llamafile\n",
    "```\n",
    "and Voila, the LLM server is running locally! Because it's running locally, \n",
    "we need to run these workflows one at a time: you'll need to run the code cell below a few times for each Llamafile you want to evaluate.\n",
    "The process will be:\n",
    "\n",
    "1. run the llamfile you want to test in a terminal window\n",
    "2. Edit the code cell below so that it reflects the model_name you are testing\n",
    "3. Run the cell, wait for it to finish\n",
    "4. Go back to the terminal window and send ctrl+c to kill the process\n",
    "\n",
    "Repeat these steps for each llamafile you want to evaluate.\n",
    "\n",
    "I'm going to evaluate a few different types of the Llama 8B model and Qwen Models.\n",
    "\n",
    "You can download them here. https://huggingface.co/collections/Bojun-Feng/deepseek-distilled-llamafiles-50b-67a471e269c04acf9aa0c79b\n",
    "\n",
    "For explanation about what each of these different suffixes mean \n",
    "(they're about quantization of gguf files), see https://github.com/ggml-org/llama.cpp/discussions/2094\n",
    "\n",
    "\n",
    "* DeepSeek-R1-Distill-Llama-8B-Q5_K_M.llamafile\n",
    "* DeepSeek-R1-Distill-Llama-8B-Q4_K_M.llamafile\n",
    "* DeepSeek-R1-Distill-Llama-8B-Q3_K_M.llamafile\n",
    "* DeepSeek-R1-Distill-Llama-8B-Q2_K_L.llamafile\n",
    "* DeepSeek-R1-Distill-Llama-8B-Q2_K.llamafile\n",
    "* DeepSeek-R1-Distill-Qwen-14B-Q2_K.llamafile\n",
    "* DeepSeek-R1-Distill-Qwen-7B-Q4_K_M.llamafile\n",
    "* DeepSeek-R1-Distill-Qwen-7B-Q2_K.llamafile\n",
    "* DeepSeek-R1-Distill-Qwen-1.5B-Q2_K.llamafile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lumigator_sdk.strict_schemas import WorkflowCreateRequest\n",
    "\n",
    "# make an eval config for a local llamafile mode\n",
    "# You will need to replace this model name with the name of whichever llamafile you're running locally.\n",
    "# This cell needs to be edited and re-run for each model you want to evaluate.\n",
    "model_name = \"DeepSeek-R1-Distill-Qwen-1.5B-Q2_K\"\n",
    "evaluation_config = create_evaluation_config(model_name=model_name, ip_address=\"localhost\", port=8080)\n",
    "\n",
    "# check if an evaluation by that name already exists\n",
    "existing_workflows = client.experiments.get_experiment(experiment_id).workflows\n",
    "existing_workflow = next(\n",
    "    (workflow for workflow in existing_workflows if workflow.name == evaluation_config[\"name\"]), None\n",
    ")\n",
    "if existing_workflow and existing_workflow.status == \"failed\":\n",
    "    client.workflows.delete_workflow(existing_workflow.id)\n",
    "    print(f\"Deleted failed workflow {evaluation_config['name']} with ID {existing_workflow.id}\")\n",
    "    existing_workflow = None\n",
    "\n",
    "if existing_workflow:\n",
    "    print(f\"Workflow {evaluation_config['name']} already exists with ID {existing_workflow.id}\")\n",
    "else:\n",
    "    request = WorkflowCreateRequest(\n",
    "        name=evaluation_config[\"name\"],\n",
    "        description=evaluation_config[\"description\"],\n",
    "        model=evaluation_config[\"model\"],\n",
    "        provider=evaluation_config[\"provider\"],\n",
    "        base_url=evaluation_config.get(\"base_url\"),\n",
    "        dataset=dataset_id,\n",
    "        experiment_id=experiment_id,\n",
    "        system_prompt=system_prompt,\n",
    "        generation_config=generation_config,\n",
    "        job_timeout_sec=60 * 60 * 2,\n",
    "        metrics=metrics,\n",
    "    )\n",
    "    client.workflows.create_workflow(request).model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing the Evaluation Workflows\n",
    "\n",
    "With all workflows now created, Lumigator will:\n",
    "\n",
    "1. Generate summaries from each model for every example in the dataset\n",
    "2. Calculate performance metrics like ROUGE, BLEU, and BERTScore\n",
    "3. Make all results available for comparison\n",
    "\n",
    "This automated evaluation approach ensures consistent testing conditions across all models. The wait_for_all_workflows function will poll the Lumigator API until all workflows complete, allowing us to retrieve and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for all workflows to complete for experiment 1\n",
      "All workflows completed!\n",
      "Average tokens in reference summaries: 241\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_c0b46_row0_col0 {\n",
       "  background-color: #087432;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row0_col1, #T_c0b46_row5_col1 {\n",
       "  background-color: #eff9eb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c0b46_row0_col2, #T_c0b46_row5_col7, #T_c0b46_row11_col4 {\n",
       "  background-color: #005020;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row0_col3, #T_c0b46_row2_col5, #T_c0b46_row3_col5, #T_c0b46_row3_col7, #T_c0b46_row3_col8 {\n",
       "  background-color: #00471c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row0_col4, #T_c0b46_row1_col4, #T_c0b46_row4_col6, #T_c0b46_row5_col5, #T_c0b46_row5_col8 {\n",
       "  background-color: #004d1f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row0_col5, #T_c0b46_row0_col6, #T_c0b46_row0_col7, #T_c0b46_row0_col8, #T_c0b46_row1_col5, #T_c0b46_row1_col7, #T_c0b46_row1_col8, #T_c0b46_row3_col2, #T_c0b46_row3_col3, #T_c0b46_row3_col4, #T_c0b46_row7_col0, #T_c0b46_row11_col0, #T_c0b46_row15_col1 {\n",
       "  background-color: #00441b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row1_col0, #T_c0b46_row3_col0, #T_c0b46_row12_col5 {\n",
       "  background-color: #147e3a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row1_col1 {\n",
       "  background-color: #f2faef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c0b46_row1_col2 {\n",
       "  background-color: #004e1f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row1_col3, #T_c0b46_row4_col2, #T_c0b46_row6_col0, #T_c0b46_row6_col8 {\n",
       "  background-color: #005522;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row1_col6, #T_c0b46_row2_col7 {\n",
       "  background-color: #00451c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row2_col0 {\n",
       "  background-color: #2c944c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row2_col1 {\n",
       "  background-color: #e2f4dd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c0b46_row2_col2 {\n",
       "  background-color: #349d53;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row2_col3 {\n",
       "  background-color: #50b264;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row2_col4 {\n",
       "  background-color: #359e53;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row2_col6, #T_c0b46_row3_col6 {\n",
       "  background-color: #00481d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row2_col8 {\n",
       "  background-color: #00491d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row3_col1 {\n",
       "  background-color: #f4fbf2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c0b46_row4_col0, #T_c0b46_row10_col5, #T_c0b46_row12_col3 {\n",
       "  background-color: #026f2e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row4_col1 {\n",
       "  background-color: #f0f9ed;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c0b46_row4_col3, #T_c0b46_row10_col8, #T_c0b46_row11_col3 {\n",
       "  background-color: #006c2c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row4_col4 {\n",
       "  background-color: #006729;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row4_col5, #T_c0b46_row4_col7, #T_c0b46_row4_col8 {\n",
       "  background-color: #004c1e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row5_col0, #T_c0b46_row6_col7, #T_c0b46_row10_col3 {\n",
       "  background-color: #005723;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row5_col2, #T_c0b46_row5_col6, #T_c0b46_row7_col8 {\n",
       "  background-color: #005221;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row5_col3, #T_c0b46_row8_col0 {\n",
       "  background-color: #004a1e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row5_col4, #T_c0b46_row8_col8, #T_c0b46_row12_col0 {\n",
       "  background-color: #005924;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row6_col1 {\n",
       "  background-color: #ecf8e8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c0b46_row6_col2, #T_c0b46_row8_col2, #T_c0b46_row8_col5, #T_c0b46_row11_col2 {\n",
       "  background-color: #005c25;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row6_col3, #T_c0b46_row8_col3, #T_c0b46_row9_col2, #T_c0b46_row12_col2 {\n",
       "  background-color: #005f26;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row6_col4, #T_c0b46_row10_col2, #T_c0b46_row12_col4 {\n",
       "  background-color: #00692a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row6_col5, #T_c0b46_row9_col4 {\n",
       "  background-color: #005622;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row6_col6, #T_c0b46_row7_col7 {\n",
       "  background-color: #005a24;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row7_col1 {\n",
       "  background-color: #f4fbf1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c0b46_row7_col2, #T_c0b46_row8_col6 {\n",
       "  background-color: #006227;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row7_col3, #T_c0b46_row7_col6, #T_c0b46_row9_col3 {\n",
       "  background-color: #005b25;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row7_col4, #T_c0b46_row14_col0 {\n",
       "  background-color: #006027;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row7_col5 {\n",
       "  background-color: #005321;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row8_col1 {\n",
       "  background-color: #f6fcf4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c0b46_row8_col4, #T_c0b46_row9_col8 {\n",
       "  background-color: #006428;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row8_col7 {\n",
       "  background-color: #005e26;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row9_col0 {\n",
       "  background-color: #29914a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row9_col1 {\n",
       "  background-color: #e9f7e5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c0b46_row9_col5 {\n",
       "  background-color: #00682a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row9_col6, #T_c0b46_row10_col4 {\n",
       "  background-color: #016e2d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row9_col7 {\n",
       "  background-color: #067230;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row10_col0 {\n",
       "  background-color: #238b45;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row10_col1 {\n",
       "  background-color: #e3f4de;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c0b46_row10_col6 {\n",
       "  background-color: #097532;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row10_col7, #T_c0b46_row11_col6 {\n",
       "  background-color: #127c39;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row11_col1, #T_c0b46_row15_col0, #T_c0b46_row15_col2, #T_c0b46_row15_col3, #T_c0b46_row15_col4, #T_c0b46_row15_col5, #T_c0b46_row15_col6, #T_c0b46_row15_col7, #T_c0b46_row15_col8 {\n",
       "  background-color: #f7fcf5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c0b46_row11_col5 {\n",
       "  background-color: #05712f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row11_col7 {\n",
       "  background-color: #157f3b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row11_col8 {\n",
       "  background-color: #006d2c;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row12_col1, #T_c0b46_row13_col1 {\n",
       "  background-color: #f2faf0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c0b46_row12_col6, #T_c0b46_row14_col2 {\n",
       "  background-color: #208843;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row12_col7 {\n",
       "  background-color: #228a44;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row12_col8 {\n",
       "  background-color: #137d39;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row13_col0 {\n",
       "  background-color: #1d8640;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row13_col2 {\n",
       "  background-color: #218944;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row13_col3 {\n",
       "  background-color: #3ba458;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row13_col4 {\n",
       "  background-color: #258d47;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row13_col5 {\n",
       "  background-color: #48ae60;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row13_col6 {\n",
       "  background-color: #53b466;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row13_col7 {\n",
       "  background-color: #62bb6d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row13_col8 {\n",
       "  background-color: #3da65a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row14_col1 {\n",
       "  background-color: #e7f6e2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c0b46_row14_col3 {\n",
       "  background-color: #2a924a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row14_col4 {\n",
       "  background-color: #17813d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_c0b46_row14_col5 {\n",
       "  background-color: #70c274;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c0b46_row14_col6 {\n",
       "  background-color: #7fc97f;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c0b46_row14_col7 {\n",
       "  background-color: #7cc87c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_c0b46_row14_col8 {\n",
       "  background-color: #68be70;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_c0b46\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_c0b46_level0_col0\" class=\"col_heading level0 col0\" >Avg Reas Tok</th>\n",
       "      <th id=\"T_c0b46_level0_col1\" class=\"col_heading level0 col1\" >Avg Answer Tok</th>\n",
       "      <th id=\"T_c0b46_level0_col2\" class=\"col_heading level0 col2\" >ROUGE-1</th>\n",
       "      <th id=\"T_c0b46_level0_col3\" class=\"col_heading level0 col3\" >ROUGE-2</th>\n",
       "      <th id=\"T_c0b46_level0_col4\" class=\"col_heading level0 col4\" >ROUGE-L</th>\n",
       "      <th id=\"T_c0b46_level0_col5\" class=\"col_heading level0 col5\" >Coherence</th>\n",
       "      <th id=\"T_c0b46_level0_col6\" class=\"col_heading level0 col6\" >Consistency</th>\n",
       "      <th id=\"T_c0b46_level0_col7\" class=\"col_heading level0 col7\" >Fluency</th>\n",
       "      <th id=\"T_c0b46_level0_col8\" class=\"col_heading level0 col8\" >Relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c0b46_level0_row0\" class=\"row_heading level0 row0\" >DeepSeek-R1-Distill-Qwen-32B</th>\n",
       "      <td id=\"T_c0b46_row0_col0\" class=\"data row0 col0\" >483.0</td>\n",
       "      <td id=\"T_c0b46_row0_col1\" class=\"data row0 col1\" >313.0</td>\n",
       "      <td id=\"T_c0b46_row0_col2\" class=\"data row0 col2\" >39.2</td>\n",
       "      <td id=\"T_c0b46_row0_col3\" class=\"data row0 col3\" >12.0</td>\n",
       "      <td id=\"T_c0b46_row0_col4\" class=\"data row0 col4\" >21.2</td>\n",
       "      <td id=\"T_c0b46_row0_col5\" class=\"data row0 col5\" >86.6</td>\n",
       "      <td id=\"T_c0b46_row0_col6\" class=\"data row0 col6\" >88.8</td>\n",
       "      <td id=\"T_c0b46_row0_col7\" class=\"data row0 col7\" >88.2</td>\n",
       "      <td id=\"T_c0b46_row0_col8\" class=\"data row0 col8\" >87.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c0b46_level0_row1\" class=\"row_heading level0 row1\" >DeepSeek-R1-Distill-Llama-70B</th>\n",
       "      <td id=\"T_c0b46_row1_col0\" class=\"data row1 col0\" >460.0</td>\n",
       "      <td id=\"T_c0b46_row1_col1\" class=\"data row1 col1\" >297.0</td>\n",
       "      <td id=\"T_c0b46_row1_col2\" class=\"data row1 col2\" >39.4</td>\n",
       "      <td id=\"T_c0b46_row1_col3\" class=\"data row1 col3\" >11.6</td>\n",
       "      <td id=\"T_c0b46_row1_col4\" class=\"data row1 col4\" >21.2</td>\n",
       "      <td id=\"T_c0b46_row1_col5\" class=\"data row1 col5\" >86.6</td>\n",
       "      <td id=\"T_c0b46_row1_col6\" class=\"data row1 col6\" >88.4</td>\n",
       "      <td id=\"T_c0b46_row1_col7\" class=\"data row1 col7\" >88.0</td>\n",
       "      <td id=\"T_c0b46_row1_col8\" class=\"data row1 col8\" >87.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c0b46_level0_row2\" class=\"row_heading level0 row2\" >DeepSeek R1</th>\n",
       "      <td id=\"T_c0b46_row2_col0\" class=\"data row2 col0\" >410.0</td>\n",
       "      <td id=\"T_c0b46_row2_col1\" class=\"data row2 col1\" >373.0</td>\n",
       "      <td id=\"T_c0b46_row2_col2\" class=\"data row2 col2\" >31.2</td>\n",
       "      <td id=\"T_c0b46_row2_col3\" class=\"data row2 col3\" >8.1</td>\n",
       "      <td id=\"T_c0b46_row2_col4\" class=\"data row2 col4\" >17.1</td>\n",
       "      <td id=\"T_c0b46_row2_col5\" class=\"data row2 col5\" >85.8</td>\n",
       "      <td id=\"T_c0b46_row2_col6\" class=\"data row2 col6\" >87.9</td>\n",
       "      <td id=\"T_c0b46_row2_col7\" class=\"data row2 col7\" >87.8</td>\n",
       "      <td id=\"T_c0b46_row2_col8\" class=\"data row2 col8\" >86.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c0b46_level0_row3\" class=\"row_heading level0 row3\" >DeepSeek-R1-Distill-Qwen-14B</th>\n",
       "      <td id=\"T_c0b46_row3_col0\" class=\"data row3 col0\" >459.0</td>\n",
       "      <td id=\"T_c0b46_row3_col1\" class=\"data row3 col1\" >284.0</td>\n",
       "      <td id=\"T_c0b46_row3_col2\" class=\"data row3 col2\" >40.3</td>\n",
       "      <td id=\"T_c0b46_row3_col3\" class=\"data row3 col3\" >12.1</td>\n",
       "      <td id=\"T_c0b46_row3_col4\" class=\"data row3 col4\" >21.6</td>\n",
       "      <td id=\"T_c0b46_row3_col5\" class=\"data row3 col5\" >85.8</td>\n",
       "      <td id=\"T_c0b46_row3_col6\" class=\"data row3 col6\" >87.8</td>\n",
       "      <td id=\"T_c0b46_row3_col7\" class=\"data row3 col7\" >87.4</td>\n",
       "      <td id=\"T_c0b46_row3_col8\" class=\"data row3 col8\" >87.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c0b46_level0_row4\" class=\"row_heading level0 row4\" >DeepSeek-R1-Distill-Qwen-14B-Q2_K</th>\n",
       "      <td id=\"T_c0b46_row4_col0\" class=\"data row4 col0\" >494.0</td>\n",
       "      <td id=\"T_c0b46_row4_col1\" class=\"data row4 col1\" >306.0</td>\n",
       "      <td id=\"T_c0b46_row4_col2\" class=\"data row4 col2\" >38.8</td>\n",
       "      <td id=\"T_c0b46_row4_col3\" class=\"data row4 col3\" >10.9</td>\n",
       "      <td id=\"T_c0b46_row4_col4\" class=\"data row4 col4\" >20.1</td>\n",
       "      <td id=\"T_c0b46_row4_col5\" class=\"data row4 col5\" >84.7</td>\n",
       "      <td id=\"T_c0b46_row4_col6\" class=\"data row4 col6\" >86.7</td>\n",
       "      <td id=\"T_c0b46_row4_col7\" class=\"data row4 col7\" >86.3</td>\n",
       "      <td id=\"T_c0b46_row4_col8\" class=\"data row4 col8\" >86.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c0b46_level0_row5\" class=\"row_heading level0 row5\" >DeepSeek-R1-Distill-Llama-8B-Q5_K_M</th>\n",
       "      <td id=\"T_c0b46_row5_col0\" class=\"data row5 col0\" >535.0</td>\n",
       "      <td id=\"T_c0b46_row5_col1\" class=\"data row5 col1\" >314.0</td>\n",
       "      <td id=\"T_c0b46_row5_col2\" class=\"data row5 col2\" >39.0</td>\n",
       "      <td id=\"T_c0b46_row5_col3\" class=\"data row5 col3\" >11.9</td>\n",
       "      <td id=\"T_c0b46_row5_col4\" class=\"data row5 col4\" >20.7</td>\n",
       "      <td id=\"T_c0b46_row5_col5\" class=\"data row5 col5\" >84.4</td>\n",
       "      <td id=\"T_c0b46_row5_col6\" class=\"data row5 col6\" >85.4</td>\n",
       "      <td id=\"T_c0b46_row5_col7\" class=\"data row5 col7\" >85.4</td>\n",
       "      <td id=\"T_c0b46_row5_col8\" class=\"data row5 col8\" >85.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c0b46_level0_row6\" class=\"row_heading level0 row6\" >DeepSeek-R1-Distill-Llama-8B</th>\n",
       "      <td id=\"T_c0b46_row6_col0\" class=\"data row6 col0\" >540.0</td>\n",
       "      <td id=\"T_c0b46_row6_col1\" class=\"data row6 col1\" >328.0</td>\n",
       "      <td id=\"T_c0b46_row6_col2\" class=\"data row6 col2\" >38.1</td>\n",
       "      <td id=\"T_c0b46_row6_col3\" class=\"data row6 col3\" >11.3</td>\n",
       "      <td id=\"T_c0b46_row6_col4\" class=\"data row6 col4\" >20.0</td>\n",
       "      <td id=\"T_c0b46_row6_col5\" class=\"data row6 col5\" >82.6</td>\n",
       "      <td id=\"T_c0b46_row6_col6\" class=\"data row6 col6\" >83.7</td>\n",
       "      <td id=\"T_c0b46_row6_col7\" class=\"data row6 col7\" >83.3</td>\n",
       "      <td id=\"T_c0b46_row6_col8\" class=\"data row6 col8\" >83.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c0b46_level0_row7\" class=\"row_heading level0 row7\" >DeepSeek-R1-Distill-Llama-8B-Q4_K_M</th>\n",
       "      <td id=\"T_c0b46_row7_col0\" class=\"data row7 col0\" >567.0</td>\n",
       "      <td id=\"T_c0b46_row7_col1\" class=\"data row7 col1\" >286.0</td>\n",
       "      <td id=\"T_c0b46_row7_col2\" class=\"data row7 col2\" >37.7</td>\n",
       "      <td id=\"T_c0b46_row7_col3\" class=\"data row7 col3\" >11.4</td>\n",
       "      <td id=\"T_c0b46_row7_col4\" class=\"data row7 col4\" >20.4</td>\n",
       "      <td id=\"T_c0b46_row7_col5\" class=\"data row7 col5\" >83.0</td>\n",
       "      <td id=\"T_c0b46_row7_col6\" class=\"data row7 col6\" >83.5</td>\n",
       "      <td id=\"T_c0b46_row7_col7\" class=\"data row7 col7\" >82.8</td>\n",
       "      <td id=\"T_c0b46_row7_col8\" class=\"data row7 col8\" >84.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c0b46_level0_row8\" class=\"row_heading level0 row8\" >DeepSeek-R1-Distill-Llama-8B-Q3_K_M</th>\n",
       "      <td id=\"T_c0b46_row8_col0\" class=\"data row8 col0\" >556.0</td>\n",
       "      <td id=\"T_c0b46_row8_col1\" class=\"data row8 col1\" >274.0</td>\n",
       "      <td id=\"T_c0b46_row8_col2\" class=\"data row8 col2\" >38.2</td>\n",
       "      <td id=\"T_c0b46_row8_col3\" class=\"data row8 col3\" >11.3</td>\n",
       "      <td id=\"T_c0b46_row8_col4\" class=\"data row8 col4\" >20.2</td>\n",
       "      <td id=\"T_c0b46_row8_col5\" class=\"data row8 col5\" >81.1</td>\n",
       "      <td id=\"T_c0b46_row8_col6\" class=\"data row8 col6\" >81.9</td>\n",
       "      <td id=\"T_c0b46_row8_col7\" class=\"data row8 col7\" >81.8</td>\n",
       "      <td id=\"T_c0b46_row8_col8\" class=\"data row8 col8\" >83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c0b46_level0_row9\" class=\"row_heading level0 row9\" >DeepSeek-R1-Distill-Llama-8B-Q2_K_L</th>\n",
       "      <td id=\"T_c0b46_row9_col0\" class=\"data row9 col0\" >417.0</td>\n",
       "      <td id=\"T_c0b46_row9_col1\" class=\"data row9 col1\" >342.0</td>\n",
       "      <td id=\"T_c0b46_row9_col2\" class=\"data row9 col2\" >37.9</td>\n",
       "      <td id=\"T_c0b46_row9_col3\" class=\"data row9 col3\" >11.4</td>\n",
       "      <td id=\"T_c0b46_row9_col4\" class=\"data row9 col4\" >20.8</td>\n",
       "      <td id=\"T_c0b46_row9_col5\" class=\"data row9 col5\" >78.4</td>\n",
       "      <td id=\"T_c0b46_row9_col6\" class=\"data row9 col6\" >79.0</td>\n",
       "      <td id=\"T_c0b46_row9_col7\" class=\"data row9 col7\" >76.5</td>\n",
       "      <td id=\"T_c0b46_row9_col8\" class=\"data row9 col8\" >80.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c0b46_level0_row10\" class=\"row_heading level0 row10\" >DeepSeek-R1-Distill-Llama-8B-Q2_K</th>\n",
       "      <td id=\"T_c0b46_row10_col0\" class=\"data row10 col0\" >430.0</td>\n",
       "      <td id=\"T_c0b46_row10_col1\" class=\"data row10 col1\" >369.0</td>\n",
       "      <td id=\"T_c0b46_row10_col2\" class=\"data row10 col2\" >37.1</td>\n",
       "      <td id=\"T_c0b46_row10_col3\" class=\"data row10 col3\" >11.5</td>\n",
       "      <td id=\"T_c0b46_row10_col4\" class=\"data row10 col4\" >19.8</td>\n",
       "      <td id=\"T_c0b46_row10_col5\" class=\"data row10 col5\" >76.9</td>\n",
       "      <td id=\"T_c0b46_row10_col6\" class=\"data row10 col6\" >76.9</td>\n",
       "      <td id=\"T_c0b46_row10_col7\" class=\"data row10 col7\" >73.2</td>\n",
       "      <td id=\"T_c0b46_row10_col8\" class=\"data row10 col8\" >78.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c0b46_level0_row11\" class=\"row_heading level0 row11\" >DeepSeek-R1-Distill-Qwen-7B</th>\n",
       "      <td id=\"T_c0b46_row11_col0\" class=\"data row11 col0\" >568.0</td>\n",
       "      <td id=\"T_c0b46_row11_col1\" class=\"data row11 col1\" >268.0</td>\n",
       "      <td id=\"T_c0b46_row11_col2\" class=\"data row11 col2\" >38.1</td>\n",
       "      <td id=\"T_c0b46_row11_col3\" class=\"data row11 col3\" >10.9</td>\n",
       "      <td id=\"T_c0b46_row11_col4\" class=\"data row11 col4\" >21.1</td>\n",
       "      <td id=\"T_c0b46_row11_col5\" class=\"data row11 col5\" >76.2</td>\n",
       "      <td id=\"T_c0b46_row11_col6\" class=\"data row11 col6\" >74.7</td>\n",
       "      <td id=\"T_c0b46_row11_col7\" class=\"data row11 col7\" >72.2</td>\n",
       "      <td id=\"T_c0b46_row11_col8\" class=\"data row11 col8\" >78.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c0b46_level0_row12\" class=\"row_heading level0 row12\" >DeepSeek-R1-Distill-Qwen-7B-Q4_K_M</th>\n",
       "      <td id=\"T_c0b46_row12_col0\" class=\"data row12 col0\" >533.0</td>\n",
       "      <td id=\"T_c0b46_row12_col1\" class=\"data row12 col1\" >293.0</td>\n",
       "      <td id=\"T_c0b46_row12_col2\" class=\"data row12 col2\" >37.9</td>\n",
       "      <td id=\"T_c0b46_row12_col3\" class=\"data row12 col3\" >10.8</td>\n",
       "      <td id=\"T_c0b46_row12_col4\" class=\"data row12 col4\" >20.0</td>\n",
       "      <td id=\"T_c0b46_row12_col5\" class=\"data row12 col5\" >72.2</td>\n",
       "      <td id=\"T_c0b46_row12_col6\" class=\"data row12 col6\" >70.9</td>\n",
       "      <td id=\"T_c0b46_row12_col7\" class=\"data row12 col7\" >68.5</td>\n",
       "      <td id=\"T_c0b46_row12_col8\" class=\"data row12 col8\" >73.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c0b46_level0_row13\" class=\"row_heading level0 row13\" >DeepSeek-R1-Distill-Qwen-7B-Q2_K</th>\n",
       "      <td id=\"T_c0b46_row13_col0\" class=\"data row13 col0\" >443.0</td>\n",
       "      <td id=\"T_c0b46_row13_col1\" class=\"data row13 col1\" >294.0</td>\n",
       "      <td id=\"T_c0b46_row13_col2\" class=\"data row13 col2\" >33.4</td>\n",
       "      <td id=\"T_c0b46_row13_col3\" class=\"data row13 col3\" >8.7</td>\n",
       "      <td id=\"T_c0b46_row13_col4\" class=\"data row13 col4\" >18.0</td>\n",
       "      <td id=\"T_c0b46_row13_col5\" class=\"data row13 col5\" >58.1</td>\n",
       "      <td id=\"T_c0b46_row13_col6\" class=\"data row13 col6\" >57.4</td>\n",
       "      <td id=\"T_c0b46_row13_col7\" class=\"data row13 col7\" >52.3</td>\n",
       "      <td id=\"T_c0b46_row13_col8\" class=\"data row13 col8\" >61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c0b46_level0_row14\" class=\"row_heading level0 row14\" >DeepSeek-R1-Distill-Qwen-1.5B</th>\n",
       "      <td id=\"T_c0b46_row14_col0\" class=\"data row14 col0\" >520.0</td>\n",
       "      <td id=\"T_c0b46_row14_col1\" class=\"data row14 col1\" >355.0</td>\n",
       "      <td id=\"T_c0b46_row14_col2\" class=\"data row14 col2\" >33.5</td>\n",
       "      <td id=\"T_c0b46_row14_col3\" class=\"data row14 col3\" >9.4</td>\n",
       "      <td id=\"T_c0b46_row14_col4\" class=\"data row14 col4\" >18.7</td>\n",
       "      <td id=\"T_c0b46_row14_col5\" class=\"data row14 col5\" >51.0</td>\n",
       "      <td id=\"T_c0b46_row14_col6\" class=\"data row14 col6\" >49.2</td>\n",
       "      <td id=\"T_c0b46_row14_col7\" class=\"data row14 col7\" >46.8</td>\n",
       "      <td id=\"T_c0b46_row14_col8\" class=\"data row14 col8\" >52.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c0b46_level0_row15\" class=\"row_heading level0 row15\" >DeepSeek-R1-Distill-Qwen-1.5B-Q2_K</th>\n",
       "      <td id=\"T_c0b46_row15_col0\" class=\"data row15 col0\" >17.0</td>\n",
       "      <td id=\"T_c0b46_row15_col1\" class=\"data row15 col1\" >1025.0</td>\n",
       "      <td id=\"T_c0b46_row15_col2\" class=\"data row15 col2\" >12.1</td>\n",
       "      <td id=\"T_c0b46_row15_col3\" class=\"data row15 col3\" >2.4</td>\n",
       "      <td id=\"T_c0b46_row15_col4\" class=\"data row15 col4\" >7.8</td>\n",
       "      <td id=\"T_c0b46_row15_col5\" class=\"data row15 col5\" >13.7</td>\n",
       "      <td id=\"T_c0b46_row15_col6\" class=\"data row15 col6\" >14.1</td>\n",
       "      <td id=\"T_c0b46_row15_col7\" class=\"data row15 col7\" >9.0</td>\n",
       "      <td id=\"T_c0b46_row15_col8\" class=\"data row15 col8\" >13.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x11de4bfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils import compile_and_display_results, get_finished_workflows\n",
    "\n",
    "print(f\"Waiting for all workflows to complete for experiment {experiment_id}\")\n",
    "# experiment = wait_for_all_workflows(client, experiment_id)\n",
    "experiment = get_finished_workflows(client, experiment_id)\n",
    "print(\"All workflows completed!\")\n",
    "workflow_details, styled_df = compile_and_display_results(client, experiment)\n",
    "display(styled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the results\n",
    "\n",
    "Now that we have our outputs and evaluations, we're free to browse through the outputs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the ground truth of an example\n",
    "example = 0\n",
    "print(\"Ground Truth of Example:\")\n",
    "print(df.iloc[example][\"ground_truth\"])\n",
    "print(\"***\" * 50)\n",
    "print(\"Converation\")\n",
    "print(df.iloc[example][\"examples\"])\n",
    "print(\"=\" * 50)\n",
    "# for a few models of interest, print them out\n",
    "for workflow in workflow_details:\n",
    "    if workflow not in [\"DeepSeek R1\", \"DeepSeek-R1-Distill-Llama-70B\", \"DeepSeek-R1-Distill-Qwen-14B-Q2_K\"]:\n",
    "        continue\n",
    "    print(f\"Model: {workflow}\")\n",
    "    print(\"==\" * 50)\n",
    "    print(workflow_details[workflow][\"artifacts\"][\"reasoning\"][example])\n",
    "    print(\"=\" * 50)\n",
    "    print(workflow_details[workflow][\"artifacts\"][\"predictions\"][example])\n",
    "    print(\"-\" * 50)\n",
    "    print(\"metrics:\")\n",
    "    print(workflow_details[workflow][\"metrics\"][\"rouge\"][\"rouge1\"][example])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Which model is best for you? With these results, \n",
    "you can gain a perspective for how the performance changes as you scale up or down.\n",
    "\n",
    "\n",
    "Based on whether you would like to use a hosted platform, self-hosted, or local model,\n",
    "you now have several options to choose from."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
