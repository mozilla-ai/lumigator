{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from time import sleep\n",
    "\n",
    "from lumigator_schemas.datasets import DatasetFormat\n",
    "from lumigator_schemas.workflows import WorkflowCreateRequest, WorkflowStatus\n",
    "from lumigator_sdk.lumigator import LumigatorClient\n",
    "from lumigator_sdk.strict_schemas import ExperimentIdCreate\n",
    "\n",
    "\n",
    "def wait_for_workflow_complete(lumi_client_int: LumigatorClient, workflow_id: str):\n",
    "    \"\"\"Wait for a workflow to complete.\"\"\"\n",
    "    workflow_details = lumi_client_int.workflows.get_workflow(workflow_id)\n",
    "    while workflow_details.status not in [WorkflowStatus.SUCCEEDED, WorkflowStatus.FAILED]:\n",
    "        sleep(5)\n",
    "        workflow_details = lumi_client_int.workflows.get_workflow(workflow_id)\n",
    "    return workflow_details\n",
    "\n",
    "\n",
    "LUMI_HOST = \"localhost:8000\"\n",
    "with Path.open(\"../lumigator/sample_data/dialogsum_exc.csv\") as file:\n",
    "    dialog_data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = LumigatorClient(api_host=LUMI_HOST)\n",
    "dataset_response = client.datasets.create_dataset(dataset=dialog_data, format=DatasetFormat.JOB)\n",
    "dataset_id = dataset_response.id\n",
    "dataset_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an experiment\n",
    "request = ExperimentIdCreate(\n",
    "    name=\"Basic Test\",\n",
    "    description=\"Let's compare a few models on my dialogsum dataset!\",\n",
    ")\n",
    "experiment_response = client.experiments.create_experiment(request)\n",
    "experiment_id = experiment_response.id\n",
    "experiment_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run the Deepseek R1 https://api-docs.deepseek.com/quick_start/pricing\n",
    "request = WorkflowCreateRequest(\n",
    "    name=\"Deepseek R1\",\n",
    "    description=\"Test workflow for inf and eval\",\n",
    "    model=\"deepseek/deepseek-reasoner\",\n",
    "    model_url=\"deepseek/deepseek-reasoner\",\n",
    "    dataset=dataset_id,\n",
    "    experiment_id=experiment_id,\n",
    "    max_samples=1,\n",
    ")\n",
    "workflow_1_response = client.workflows.create_workflow(request)\n",
    "workflow_1_id = workflow_1_response.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait till the workflow is done\n",
    "workflow_1_details = wait_for_workflow_complete(client, workflow_1_id)\n",
    "# pretty print the output as json\n",
    "workflow_1_details.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the results of the experiment\n",
    "experiment_results = client.experiments.get_experiment(experiment_id)\n",
    "experiment_results.model_dump()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
