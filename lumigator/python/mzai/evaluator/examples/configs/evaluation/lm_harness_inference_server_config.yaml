name: "lm-buddy-lm-harness-inference"

# Model to evaluate specified as a local-chat-completions inference server
model:
  inference:
    base_url: "http://1.2.3.4:8000/v1"
    # HuggingFace repo for the engine model being hosted
    engine: "hf://distilgpt2"
    # # W&B artifact can also be specified as the engine model to generate a lineage
    # engine: "wandb://sample-entity/lm-buddy-examples/name:latest"
  # 'huggingface' or 'tiktoken' depending on model type
  tokenizer_backend: "huggingface"

# Settings specific to lm_harness.evaluate
evaluation:
  tasks: ["gsm8k"]
  num_fewshot: 5
  limit: 10

tracking:
  project: "lm-buddy-examples"
  entity: "sample"
