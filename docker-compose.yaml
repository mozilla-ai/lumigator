name: lumigator

services:

  minio:
    image: quay.io/minio/minio:RELEASE.2024-12-18T13-15-44Z
    command: server /data --console-address ":9001"
    ports:
      - 9000:9000
      - 9001:9001
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 5s
      timeout: 20s
      retries: 18
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
      - MINIO_API_CORS_ALLOW_ORIGIN=*
    volumes:
    # - ${HOME}/minio/data:/data
      - minio-data:/data
    profiles:
      - local

  minio-admin:
    image: quay.io/minio/minio:RELEASE.2024-12-18T13-15-44Z
    depends_on:
      minio:
        condition: service_healthy
    entrypoint:
      - /bin/bash
      - -c
      - |
          set -ex
          mc alias set lumigator_s3 http://minio:9000 minioadmin minioadmin
          mc admin user add lumigator_s3 lumigator lumigator
          mc admin policy attach lumigator_s3 readwrite --user lumigator
          mc mb -p lumigator_s3/lumigator-storage
    extra_hosts:
      - "localhost:host-gateway"
    profiles:
      - local

  redis:
    image: redis:8.0-M03-alpine
    command: redis-server --save 60 1 --loglevel warning
    profiles:
      - local
    volumes:
      - redis-data:/data
    healthcheck:
      test: ["CMD-SHELL", "redis-cli ping"]
      interval: 1s
      timeout: 3s
      retries: 5

  ray:
    image: rayproject/ray:2.30.0-py311${COMPUTE_TYPE}${RAY_ARCH_SUFFIX}
    depends_on:
      redis:
        condition: service_healthy
    ports:
      - "6379:6379"
      - "8265:8265"
      - "10001:10001"
    # https://docs.ray.io/en/releases-2.30.0/cluster/cli.html#ray-start for more info about the command
    entrypoint:
      - /bin/bash
      - -c
      - |
          set -eaux
          mkdir -p /tmp/ray_pip_cache
          RAY_REDIS_ADDRESS=redis:6379 ray start --head --dashboard-port=8265 --port=6379 --dashboard-host=0.0.0.0 --ray-client-server-port 10001
          mkdir -p /tmp/ray/session_latest/runtime_resources/pip
          rmdir /tmp/ray/session_latest/runtime_resources/pip/ && ln -s /tmp/ray_pip_cache /tmp/ray/session_latest/runtime_resources/pip
          sleep infinity
    shm_size: 2g
    volumes:
      - ${HOME}/.cache/huggingface:/home/ray/.cache/huggingface
      - ray-pip-cache:/tmp/ray_pip_cache
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: '5g'
    environment:
      # LOCAL_FSSPEC_S3 env vars required by s3fs running inside evaluator ray jobs
      - LOCAL_FSSPEC_S3_ENDPOINT_URL=${AWS_ENDPOINT_URL} # Should match AWS_ENDPOINT_URL
      - LOCAL_FSSPEC_S3_KEY=${AWS_ACCESS_KEY_ID} # Should match AWS_SECRET_ACCESS_KEY
      - LOCAL_FSSPEC_S3_SECRET=${AWS_SECRET_ACCESS_KEY} # Should match AWS_SECRET_ACCESS_KEY
      - MISTRAL_API_KEY
      - OPENAI_API_KEY
      - HF_TOKEN
      - AWS_ACCESS_KEY_ID
      - AWS_SECRET_ACCESS_KEY
      - AWS_DEFAULT_REGION
      - AWS_ENDPOINT_URL

    # NOTE: to keep AWS_ENDPOINT_URL as http://localhost:9000 both on the host system
    #       and inside containers, we map localhost to the host gateway IP.
    #       This currently works properly, but might be the cause of networking
    #       issues down the line. This should be used only for local, development
    #       deployments.
    extra_hosts:
      - "localhost:host-gateway"
    profiles:
      - local

  backend:
    image: mzdotai/lumigator:v0.1.0-alpha
    build:
      context: .
      dockerfile: "Dockerfile"
      target: "main_image"
    depends_on:
      minio-admin:
        condition: service_completed_successfully
      minio:
        condition: "service_started"
        required: false
      ray:
        condition: "service_started"
        required: false
    ports:
      - 8000:8000
    environment:
      - DEPLOYMENT_TYPE=local
      # The local file needs to be available through a mount,
      # if persistence is needed
      - SQLALCHEMY_DATABASE_URL=sqlite:///local.db
      - S3_ENDPOINT_URL=${AWS_ENDPOINT_URL}
      - AWS_ACCESS_KEY_ID
      - AWS_SECRET_ACCESS_KEY
      - AWS_DEFAULT_REGION
      - AWS_ENDPOINT_URL
      - S3_BUCKET
      - EVALUATOR_PIP_REQS=/mzai/lumigator/jobs/evaluator/requirements.txt
      - EVALUATOR_WORK_DIR=/mzai/lumigator/jobs/evaluator
      # TODO: the following two rows should be renamed to EVALUATOR_*
      #       and the two above should be removed when we depreate evaluator
      - EVALUATOR_LITE_PIP_REQS=/mzai/lumigator/jobs/evaluator_lite/requirements.txt
      - EVALUATOR_LITE_WORK_DIR=/mzai/lumigator/jobs/evaluator_lite
      - INFERENCE_PIP_REQS=/mzai/lumigator/jobs/inference/requirements.txt
      - INFERENCE_WORK_DIR=/mzai/lumigator/jobs/inference
      - RAY_DASHBOARD_PORT
      - RAY_HEAD_NODE_HOST
      - MISTRAL_API_KEY=$MISTRAL_API_KEY
      - OPENAI_API_KEY=$OPENAI_API_KEY
      - RAY_WORKER_GPUS=$RAY_WORKER_GPUS
      - RAY_WORKER_GPUS_FRACTION=$RAY_WORKER_GPUS_FRACTION
      - LUMI_API_CORS_ALLOWED_ORIGINS
    # NOTE: to keep AWS_ENDPOINT_URL as http://localhost:9000 both on the host system
    #       and inside containers, we map localhost to the host gateway IP.
    #       This currently works properly, but might be the cause of networking
    #       issues down the line. This should be used only for local, development
    #       deployments.
    extra_hosts:
      - "localhost:host-gateway"

  frontend:
    image: mzdotai/lumigator-frontend:v0.1.0-alpha
    build:
      context: .
      dockerfile: "./lumigator/frontend/Dockerfile"
      target: "server"
      args:
       VUE_APP_BASE_URL: http://localhost/api/v1/
    environment:
      LUMIGATOR_API_PORT: 8000
      LUMIGATOR_API_HOST: backend
    volumes:
      - ./lumigator/frontend/nginx/:/etc/nginx/templates/
    depends_on:
      backend:
        condition: "service_started"
        required: true
    ports:
      - 80:80

volumes:
    minio-data:
    database_volume:
<<<<<<< HEAD
    redis-data:
    ray-pip-cache:
=======
>>>>>>> 658f734e (Setup pip cache during CI)
