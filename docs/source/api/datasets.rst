Datasets API
============

A dataset is a collection of data points (or samples) used for training, testing, or evaluating machine learning models.

An **evaluation** dataset specifically assesses a model's performance on a particular task.

Why bother creating your own
-----------------------------

Unlike generic, open source benchmarks, a curated dataset can reflect the nuances, vocabulary, and challenges unique to the use case. Intuitively, summarizing patient information will require a different vocabulary than summarizing a film. Even within the same domain, some models may be enough to handle short technical messages, whereas they might struggle with long technical papers.

Would that model also work for you better than a generic LLM? Would you really lose performance if you used a smaller model? Those are questions Lumigator can help you answer for your use case. The emphasis here is on "for your use case"; for an LLM, data is what best defines a use case.

What is a good dataset?
------------------------

A good way to think of this is through the analogy of designing a good exam for a class. Will it cover the syllabus completely? Is it long enough to avoid the odd (un)lucky answer having too much weight? For each answer you include in your exam, do you know what the ideal answer would be, so that you can compare students' answers to it?

Content
-------

At its simplest, an evaluation dataset for a specific task should contain the following key components:

* Input text or examples: These are the samples that the model will process; e.g. each individual text from your domain that a model should summarize.
* Ground truth: Answers that you would deem correct for each input. Ideally these are generated by humans who are an expert in the topic.

Ideally, you could also add metadata in additional columns that detail the source, date of collection or level of difficulty of each sample. However, Lumigator does not use this additional metadata.


I'm convinced, how do I prepare one?
-------------------------------------

Follow the guide on `Preparing your own evaluation dataset <../user-guides/prepare-evaluation-dataset.md>`_.



Example
-------

Below is an example of how to turn a huggingface dataset into one that is compatible with Lumigator.

.. code-block:: python

  from datasets import load_dataset
  from lumigator_schemas.datasets import DatasetFormat
  from lumigator_sdk.lumigator import LumigatorClient

  # First, grab the dataset off huggingface: https://huggingface.co/datasets/YuanPJ/summ_screen
  ds = load_dataset("YuanPJ/summ_screen", "fd")["test"]

  # Now let's prepare it for Lumigator upload. We need to rename some columns and delete the rest
  # rename the column "input" to "text" and "output" to "ground_truth". This is what Lumigator expects
  ds = ds.rename_column("Transcript", "examples")
  ds = ds.rename_column("Recap", "ground_truth")

  # remove all columns except "text" and "ground_truth"
  columns_list = ds.column_names
  columns_list.remove("examples")
  columns_list.remove("ground_truth")
  ds = ds.remove_columns(columns_list)

  # convert ds to a csv and make it a string so we can upload it with the Lumigator API
  DS_OUTPUT = "summ_screen.csv"
  ds.to_csv(DS_OUTPUT)

  # Time to connect up to the Lumigator client!
  LUMI_HOST = "localhost:8000"
  client = LumigatorClient(api_host=LUMI_HOST)

  # Upload that file that we created earlier
  with Path.open(DS_OUTPUT) as file:
      data = file.read()
  dataset_response = client.datasets.create_dataset(dataset=data, format=DatasetFormat.JOB)


The API provides endpoints to list all datasets, retrieve details of a specific dataset,
delete a dataset, and download datasets.
The upload process ensures that the dataset is compatible with HuggingFace standards,
although the recreated CSV file may have different delimiters.
The API supports various operations with appropriate status codes to indicate the success or failure of each request.

Endpoints
---------

.. openapi:: ../specs/openapi.json
   :include:
     /api/v1/datasets/*
