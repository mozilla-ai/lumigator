{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-Shot Evaluation of DeepSeek R1 Models for Clinical Conversation Summarization\n",
    "\n",
    "## Background and Motivation\n",
    "\n",
    "The DeepSeek family of models represents an interesting advancement in reasoning-specialized language models. While DeepSeek published evaluation results in [their paper](https://arxiv.org/pdf/2501.12948) (see Table 5), I wanted to understand specifically how the various Distilled R1 models compare to the full R1 model on a practical use case: clinical conversation summarization using the ACI-Bench dataset.\n",
    "\n",
    "This notebook demonstrates how to use **Lumigator** to systematically evaluate and compare these models. Lumigator provides a framework to:\n",
    "\n",
    "1. Coordinate multiple model evaluations against the same dataset\n",
    "2. Execute inference requests across different model deployments\n",
    "3. Calculate standardized metrics for performance comparison\n",
    "4. Organize and visualize the results for analysis\n",
    "\n",
    "## Getting Started with Lumigator\n",
    "\n",
    "To use this notebook, you'll need to have Lumigator running. In a terminal, run:\n",
    "\n",
    "```bash\n",
    "git clone git@github.com:mozilla-ai/lumigator.git\n",
    "cd lumigator\n",
    "make setup\n",
    "echo $DEEPSEEK_API_KEY # This shouldn't be empty\n",
    "echo $OPENAI_API_KEY # This shouldn't be empty, you need it for G-Eval metric\n",
    "make start-lumigator-build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lumigator_sdk.lumigator import LumigatorClient\n",
    "\n",
    "# Time to connect up to the Lumigator client!\n",
    "LUMI_HOST = \"localhost:8000\"\n",
    "client = LumigatorClient(api_host=LUMI_HOST)\n",
    "print(f\"Connection is: {client.health.healthcheck().status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: ACI-Bench for Clinical Documentation\n",
    "\n",
    "This evaluation uses the ACI-Bench dataset, which was introduced in the paper \n",
    "[\"ACI-Bench: a Novel Benchmark for Ambient Clinical Intelligence\"](https://www.nature.com/articles/s41597-023-02487-3) \n",
    "(Yim et al., 2023). \n",
    "\n",
    "ACI-Bench was specifically designed to evaluate AI systems on their ability to \n",
    "understand doctor-patient conversations and generate accurate clinical documentation.\n",
    "\n",
    "### About the Dataset\n",
    "\n",
    "The test split of ACI-Bench that we'll be using consists of 40 doctor-patient conversations. \n",
    "These conversations aren't from real patient encounters but were created through professional medical simulations \n",
    "with standardized patients (actors trained to portray patients) and licensed physicians. \n",
    "\n",
    "This approach attempts to keep the data reasonably realistic while also being HIPAA-compliant, \n",
    "as no actual protected health information is included.\n",
    "\n",
    "Each conversation includes:\n",
    "\n",
    "1. A full transcript of the simulated clinical encounter, with speaker identification\n",
    "2. Human-written reference documentation\n",
    "3. Various sections of the standard clinical note format (SOAP - Subjective, Objective, Assessment, Plan)\n",
    "\n",
    "### The Assessment & Plan Task\n",
    "\n",
    "In this evaluation, we're specifically working with the **assessment and plan section** (`clef_taskC_test3_assessment_and_plan.json`), which is particularly challenging as it requires:\n",
    "\n",
    "- Identifying the patient's medical conditions\n",
    "- Understanding the physician's diagnostic reasoning\n",
    "- Summarizing the recommended treatment approach\n",
    "- Capturing follow-up plans and contingencies\n",
    "\n",
    "This section of clinical documentation represents higher-level medical reasoning compared to other sections, making it a interesting test of a model's capacity for complex medical summarization and inference.\n",
    "\n",
    "Each example in our dataset contains:\n",
    "- `examples`: The full doctor-patient conversation transcript (with speaker turns marked as `[doctor]` and `[patient]`)\n",
    "- `ground_truth`: The human-written assessment and plan section that serves as the reference summary\n",
    "- `id`: A unique identifier for each conversation\n",
    "\n",
    "The Assessment & Plan task was featured in the 2023 MEDIQA-CHAT shared task at CLEF (Conference and Labs of the Evaluation Forum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of This Evaluation\n",
    "\n",
    "This evaluation has several important limitations that should be considered when interpreting the results:\n",
    "\n",
    "1. **Unknown Training Data Exposure**: We cannot verify whether DeepSeek models were trained on the ACI-Bench dataset or similar clinical conversations. If any of these models were exposed to this data during training, they would have an unfair advantage in this evaluation - essentially having already \"seen the answers\" to the test. Without model cards or detailed training information disclosing training datasets, this remains an unknown factor.\n",
    "\n",
    "2. **Relative Comparison Focus**: Given this limitation, our analysis primarily focuses on the relative performance differences between models within the DeepSeek family, rather than making absolute claims about their capabilities for clinical summarization. By comparing models from the same family, we can still draw meaningful conclusions about how performance scales with model size and architecture (Llama vs. Qwen) when all models would have had the same potential exposure to training data.\n",
    "\n",
    "3. **Single Task Evaluation**: This evaluation examines performance on just one specific clinical documentation task (Assessment & Plan generation) and may not generalize to other medical tasks or to clinical summarization in different specialties or contexts.\n",
    "\n",
    "4. **Simulated Data**: While the ACI-Bench dataset uses realistic simulated conversations, model performance might differ on real-world clinical conversations, which tend to be messier, less structured, and potentially contain more specialized terminology.\n",
    "\n",
    "5. **Zero-Shot Setting**: Our evaluation uses a zero-shot approach with a specialized system prompt. \n",
    "Performance might improve significantly with few-shot examples.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "# GitHub API URL to fetch the file list\n",
    "download_url = \"https://raw.githubusercontent.com/wyim/aci-bench/main/data/challenge_data_json/clef_taskC_test3_assessment_and_plan.json\"\n",
    "file_name = download_url.split(\"/\")[-1]\n",
    "save_dir = Path(\"data\")\n",
    "file_path = save_dir / file_name\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "response = requests.get(download_url)\n",
    "\n",
    "data = response.json()\n",
    "# convert it to a dataframe. The file by default has the columns 'src' and 'tgt'\n",
    "df = pd.DataFrame(data[\"data\"])  # noqa: PD901\n",
    "# Rename the columns to \"examples\" and \"ground_truth\", which is what the Lumigator API expects for the data\n",
    "df = df.rename(columns={\"src\": \"examples\", \"tgt\": \"ground_truth\", \"file\": \"id\"})  # noqa: PD901\n",
    "\n",
    "processed_file_path = file_path.with_suffix(\".csv\")\n",
    "# save it as a csv\n",
    "df.to_csv(processed_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now the data is all formatted: let's take a look at an example to get a feel for what the data looks like. \n",
    "Understanding the data is crucial for interpreting the results and behavior of the models being evaluated. \n",
    "\n",
    "Every dataset\n",
    "has quirks and unique things about it: in this notebook we won't dive too deeply into investigating the characteristics of the dataset,\n",
    "but it's definitely worth taking more time to understand exactly what is in a dataset before you use it for anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.iloc[0]\n",
    "print(\"--- Snippet of Conversation ---\")\n",
    "print(\"\\n\".join(sample[\"examples\"].split(\"\\n\")[6:8]))\n",
    "print(\" --- Assessment & Plan---\")\n",
    "print(sample[\"ground_truth\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Dataset into Lumigator\n",
    "Now, let's upload the dataset into lumigator using the Lumigator SDK. creating the dataset returns the dataset ID, which we will attach to future requests so that Lumigator knows which dataset should be used for running an eval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from lumigator_schemas.datasets import DatasetFormat\n",
    "\n",
    "# Upload that file that we created earlier\n",
    "with Path.open(Path(processed_file_path), \"r\") as file:\n",
    "    data = file.read()\n",
    "dataset_response = client.datasets.create_dataset(dataset=data, format=DatasetFormat.JOB)\n",
    "dataset_id = dataset_response.id\n",
    "print(f\"Dataset uploaded and has ID: {dataset_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Evaluation Pipeline in Lumigator\n",
    "\n",
    "Now that we've uploaded our dataset, we'll create an experiment in Lumigator. In Lumigator terminology:\n",
    "\n",
    "1. **Experiment** - A container that organizes related evaluation workflows\n",
    "2. **Workflow** - A specific model configuration being evaluated against the dataset\n",
    "3. **Dataset** - The collection of examples (in our case, clinical conversations)\n",
    "\n",
    "This structure allows us to compare multiple models on the same dataset in a systematic way, with all results organized within a single experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now time to create an experiment in Lumigator! This is a container for all the workflows we'll run\n",
    "from lumigator_schemas.experiments import ExperimentCreate\n",
    "\n",
    "experiment_id = input(\"Enter the experiment ID, or press enter to create a new experiment: \")\n",
    "if not experiment_id:\n",
    "    request = ExperimentCreate(\n",
    "        name=\"ACI-Bench clef_taskC_test3_assessment_and_plan\",\n",
    "        description=\"https://github.com/wyim/aci-bench/tree/main\",\n",
    "        dataset=dataset_id,\n",
    "    )\n",
    "    experiment_response = client.experiments.create_experiment(request)\n",
    "    experiment_id = experiment_response.id\n",
    "    print(f\"Experiment created and has ID: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection Rationale\n",
    "\n",
    "For this evaluation, we're testing a range of DeepSeek models to understand how performance scales with model size and architecture:\n",
    "\n",
    "- **DeepSeek R1** - The original reasoning-specialized model\n",
    "- **DeepSeek-R1-Distill-Llama** variants (8B and 70B) - Knowledge distilled into Llama architecture\n",
    "- **DeepSeek-R1-Distill-Qwen** variants (1.5B to 32B) - Knowledge distilled into Qwen architecture\n",
    "\n",
    "This selection allows us to analyze:\n",
    "1. How model size affects clinical summarization quality\n",
    "2. Whether the base architecture (Llama vs Qwen) impacts performance\n",
    "3. What performance tradeoffs come with using smaller distilled models\n",
    "\n",
    "The smaller distilled models could be particularly valuable in resource-constrained clinical settings if they maintain adequate performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying Models for the DeepSeek Evaluation\n",
    "\n",
    "To fully execute this notebook, you'll need to deploy the DeepSeek models yourself so that Lumigator can access them:\n",
    "\n",
    "1. **Set up model deployments** for the DeepSeek models (both Llama and Qwen variants)\n",
    "2. **Configure your `.env` file** with the IP addresses of your deployed models:\n",
    "   ```\n",
    "   # Llama models\n",
    "   LLAMA_8B_IP=<your-deployment-ip>\n",
    "   LLAMA_70B_IP=<your-deployment-ip>\n",
    "   \n",
    "   # Qwen models\n",
    "   QWEN_1_5B_IP=<your-deployment-ip>\n",
    "   QWEN_7B_IP=<your-deployment-ip>\n",
    "   QWEN_14B_IP=<your-deployment-ip>\n",
    "   QWEN_32B_IP=<your-deployment-ip>\n",
    "   ```\n",
    "\n",
    "For detailed instructions on how to deploy DeepSeek models on Kubernetes, see the guide on the Mozilla.ai blog: [Deploying DeepSeek V3 on Kubernetes](https://blog.mozilla.ai/deploying-deepseek-v3-on-kubernetes/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are all the models we want to evaluate\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from utils import create_evaluation_config\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "evaluations = [\n",
    "    # Note that you need to have run Lumigator with the DEEPSEEK_API_KEY environment variable set,\n",
    "    # so that the Lumigator server can access the DeepSeek API\n",
    "    {\n",
    "        \"name\": \"DeepSeek R1\",\n",
    "        \"description\": \"DeepSeek R1 https://api-docs.deepseek.com/quick_start/pricing\",\n",
    "        \"model\": \"deepseek-reasoner\",\n",
    "        \"provider\": \"deepseek\",\n",
    "    },\n",
    "    # vLLM deployments - Llama models\n",
    "    create_evaluation_config(model_name=\"DeepSeek-R1-Distill-Llama-8B\", ip_address=os.getenv(\"LLAMA_8B_IP\")),\n",
    "    create_evaluation_config(model_name=\"DeepSeek-R1-Distill-Llama-70B\", ip_address=os.getenv(\"LLAMA_70B_IP\")),\n",
    "    # # vLLM deployments - Qwen models\n",
    "    create_evaluation_config(model_name=\"DeepSeek-R1-Distill-Qwen-7B\", ip_address=os.getenv(\"QWEN_7B_IP\")),\n",
    "    create_evaluation_config(model_name=\"DeepSeek-R1-Distill-Qwen-14B\", ip_address=os.getenv(\"QWEN_14B_IP\")),\n",
    "    create_evaluation_config(model_name=\"DeepSeek-R1-Distill-Qwen-32B\", ip_address=os.getenv(\"QWEN_32B_IP\")),\n",
    "    create_evaluation_config(model_name=\"DeepSeek-R1-Distill-Qwen-1.5B\", ip_address=os.getenv(\"QWEN_1_5B_IP\")),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance of the Custom System Prompt for Clinical Summarization\n",
    "\n",
    "The custom system prompt is critical to the clinical conversation summarization task for several reasons:\n",
    "\n",
    "1. **Domain-specific guidance**: By specifying that the model should act as an \"expert medical scribe,\" we establish the specialized knowledge domain and expected level of expertise.\n",
    "\n",
    "2. **Task definition**: The prompt clearly defines the task of converting conversational medical dialogue into a structured Assessment & Plan (A&P) document, which requires significant information distillation and reorganization.\n",
    "\n",
    "3. **Format standardization**: The instruction to create \"problem oriented\" summaries in \"narrative paragraph form\" ensures consistent outputs across all model evaluations, making comparisons more meaningful.\n",
    "\n",
    "4. **Clinical comprehensiveness**: By explicitly requesting information about \"medical treatment, patient consent, patient education and counseling, and medical reasoning,\" the prompt ensures the models capture all critical components of medical documentation.\n",
    "\n",
    "5. **Zero-shot performance**: Without this prompt, models would lack the context necessary to produce clinically useful summaries, especially the smaller distilled models being evaluated.\n",
    "\n",
    "6. **Bias reduction**: The consistent prompt reduces variability in how different models interpret the task, allowing for more direct comparison of their inherent capabilities in medical summarization.\n",
    "\n",
    "This prompt essentially serves as a controlled variable in our experiment, allowing us to focus on how different DeepSeek model variants perform on the same well-defined clinical task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an expert medical scribe who is tasked with reading the transcript of a conversation between a doctor and a patient,\n",
    "and generating a concise Assessment & Plan (A&P) summary. \n",
    "Please follow the best standards and practices for modern scribe documentation.\n",
    "The A&P should be problem oriented, with the assessment being a short narrative and the plan being a list with nested bullets.\n",
    "When appropriate, please include information about medical treatment, patient consent, patient education and counseling, and medical reasoning.\n",
    "\"\"\".strip()  # noqa: E501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lumigator_sdk.strict_schemas import WorkflowCreateRequest\n",
    "\n",
    "# Configure generation parameters to ensure deterministic, high-quality outputs\n",
    "# - temperature=0.0: Makes output deterministic (no randomness)\n",
    "# - top_p=0.9: Limits token selection to the most probable ones\n",
    "# - max_new_tokens=1024: Caps response length appropriately for reasoning + clinical summaries\n",
    "# - frequency_penalty=0.0: No penalty for token repetition\n",
    "generation_config = {\n",
    "    \"temperature\": 0.0,\n",
    "    \"top_p\": 0.9,\n",
    "    \"max_new_tokens\": 1024,\n",
    "    \"frequency_penalty\": 0.0,\n",
    "}\n",
    "\n",
    "# Create a workflow for each model configuration in our evaluation list\n",
    "# Each workflow represents a single model's inference evaluation against the dataset\n",
    "# within the experiment, allowing for systematic comparison of results\n",
    "for evaluation_config in evaluations:\n",
    "    # check if an evaluation by that name already exists\n",
    "    existing_workflows = client.experiments.get_experiment(experiment_id).workflows\n",
    "    existing_workflow = next(\n",
    "        (workflow for workflow in existing_workflows if workflow.name == evaluation_config[\"name\"]), None\n",
    "    )\n",
    "    if existing_workflow:\n",
    "        # if status is failed, delete it\n",
    "        if existing_workflow.status == \"failed\":\n",
    "            client.workflows.delete_workflow(existing_workflow.id)\n",
    "            print(f\"Deleted failed workflow {evaluation_config['name']} with ID {existing_workflow.id}\")\n",
    "        else:\n",
    "            print(f\"Workflow {evaluation_config['name']} already exists with ID {existing_workflow.id}\")\n",
    "            continue\n",
    "    request = WorkflowCreateRequest(\n",
    "        name=evaluation_config[\"name\"],\n",
    "        description=evaluation_config[\"description\"],\n",
    "        model=evaluation_config[\"model\"],\n",
    "        provider=evaluation_config[\"provider\"],\n",
    "        base_url=evaluation_config.get(\"base_url\"),\n",
    "        dataset=dataset_id,\n",
    "        experiment_id=experiment_id,\n",
    "        system_prompt=system_prompt,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "    created_workflow = client.workflows.create_workflow(request)\n",
    "    print(f\"Created workflow {created_workflow.name} with ID {created_workflow.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llamafile Workflows\n",
    "\n",
    "In addition to all the DeepSeek models that are running remotely in DeepSeek or our own vLLM deployment, \n",
    "let's also compare how local models run with Llamafile stack up! We'll try a few different ones, conveniently available for \n",
    "us at https://huggingface.co/collections/Bojun-Feng/deepseek-distilled-llamafiles-50b-67a471e269c04acf9aa0c79b.\n",
    "\n",
    "The amazing thing about llamafile is how simple it is! It's build on top of Llama.cpp, and using it is as simple as\n",
    "downloading the file, opening up a terminal, and running:\n",
    "\n",
    "```bash\n",
    "$ chmod +x <file_name>.llamafile\n",
    "$ ./<file_name>.llamafile\n",
    "```\n",
    "and Voila, the LLM server is running locally! Because it's running locally, \n",
    "we need to run these workflows one at a time: you'll need to run the code cell below a few times for each Llamafile you want to evaluate.\n",
    "The process will be:\n",
    "\n",
    "1. run the llamfile you want to test in a terminal window\n",
    "2. Edit the code cell below so that it reflects the model_name you are testing\n",
    "3. Run the cell, wait for it to finish\n",
    "4. Go back to the terminal window and send ctrl+c to kill the process\n",
    "\n",
    "Repeat these steps for each llamafile you want to evaluate.\n",
    "\n",
    "I'm going to evaluate a few different types of the Llama 8B model. https://huggingface.co/Bojun-Feng/DeepSeek-R1-Distill-Llama-8B-GGUF-llamafile\n",
    "\n",
    "For explanation about what each of these different suffixes mean \n",
    "(they're about quantization of gguf files), see https://github.com/ggml-org/llama.cpp/discussions/2094\n",
    "\n",
    "* DeepSeek-R1-Distill-Llama-8B-Q2_K.llamafile\n",
    "* DeepSeek-R1-Distill-Llama-8B-Q2_K_L.llamafile\n",
    "* DeepSeek-R1-Distill-Llama-8B-Q4_K_M.llamafile\n",
    "* DeepSeek-R1-Distill-Llama-8B-Q5_K_M.llamafile\n",
    "* DeepSeek-R1-Distill-Llama-8B-Q6_K.llamafile\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lumigator_sdk.strict_schemas import WorkflowCreateRequest\n",
    "\n",
    "# make an eval config for a local llamafile mode\n",
    "model_name = \"DeepSeek-R1-Distill-Llama-8B-Q8_0\"\n",
    "evaluation_config = create_evaluation_config(model_name=model_name, ip_address=\"localhost\", port=8080)\n",
    "\n",
    "# check if an evaluation by that name already exists\n",
    "existing_workflows = client.experiments.get_experiment(experiment_id).workflows\n",
    "existing_workflow = next(\n",
    "    (workflow for workflow in existing_workflows if workflow.name == evaluation_config[\"name\"]), None\n",
    ")\n",
    "if existing_workflow and existing_workflow.status == \"failed\":\n",
    "    client.workflows.delete_workflow(existing_workflow.id)\n",
    "    print(f\"Deleted failed workflow {evaluation_config['name']} with ID {existing_workflow.id}\")\n",
    "    existing_workflow = None\n",
    "\n",
    "if existing_workflow:\n",
    "    print(f\"Workflow {evaluation_config['name']} already exists with ID {existing_workflow.id}\")\n",
    "else:\n",
    "    request = WorkflowCreateRequest(\n",
    "        name=evaluation_config[\"name\"],\n",
    "        description=evaluation_config[\"description\"],\n",
    "        model=evaluation_config[\"model\"],\n",
    "        provider=evaluation_config[\"provider\"],\n",
    "        base_url=evaluation_config.get(\"base_url\"),\n",
    "        dataset=dataset_id,\n",
    "        experiment_id=experiment_id,\n",
    "        system_prompt=system_prompt,\n",
    "        generation_config=generation_config,\n",
    "        job_timeout_sec=60 * 60 * 2,\n",
    "    )\n",
    "    client.workflows.create_workflow(request).model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing the Evaluation Workflows\n",
    "\n",
    "With all workflows now created, Lumigator will:\n",
    "\n",
    "1. Generate summaries from each model for every example in the dataset\n",
    "2. Calculate performance metrics like ROUGE, BLEU, and BERTScore\n",
    "3. Make all results available for comparison\n",
    "\n",
    "This automated evaluation approach ensures consistent testing conditions across all models. The wait_for_all_workflows function will poll the Lumigator API until all workflows complete, allowing us to retrieve and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import compile_and_display_results, wait_for_all_workflows\n",
    "\n",
    "print(f\"Waiting for all workflows to complete for experiment {experiment_id}\")\n",
    "experiment = wait_for_all_workflows(client, experiment_id)\n",
    "print(\"All workflows completed!\")\n",
    "workflow_details, styled_df = compile_and_display_results(client, experiment)\n",
    "display(styled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the ground truth of an example\n",
    "example = 1\n",
    "print(\"Ground Truth of Example:\")\n",
    "print(df.iloc[example][\"ground_truth\"])\n",
    "print(\"=\" * 50)\n",
    "# for each model print its name and prediction\n",
    "for workflow in workflow_details:\n",
    "    print(f\"Model: {workflow}\")\n",
    "    print(\"==\" * 50)\n",
    "    print(workflow_details[workflow][\"artifacts\"][\"reasoning\"][example])\n",
    "    print(\"=\" * 50)\n",
    "    print(workflow_details[workflow][\"artifacts\"][\"predictions\"][example])\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
