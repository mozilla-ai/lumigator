{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Deepseek/Mistral/OpenAI Models using Lumigator 🐊\n",
    "\n",
    "There's been a lot of hype around [Deepseek R-1](https://github.com/deepseek-ai/DeepSeek-R1): \n",
    "it's an open source model that rivals OpenAIs o1 performance!\n",
    "\n",
    "In this notebook, we will use Lumigator in order to evaluate Deepseek R-1 against OpenAI o1 and Mistral Large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Neither GPT-4o or DeepSeek R-1 have published exactly what data they've used for training. This means\n",
    "that anything on the internet may have been used in its training and we have no way to verify that \n",
    "any public data wasn't used in the training process. This makes any evaluation we do with public data\n",
    "inherently flawed. If we post it on the internet, it's technically possible to use for LLM training and\n",
    "is no longer a reliable benchmark for future models. \n",
    "\n",
    "That's a big caveat to this notebook demonstration: the model performance differences don't actually \n",
    "indicate which model is better, in order to answer that question you'll have to try it on your own data\n",
    "that couldn't possibly have been used for training DeepSeek R-1 or GPT-4o!\n",
    "\n",
    "With that in mind, the dataset we'll use here is called [SummScreen](https://arxiv.org/abs/2104.07091) ForeverDreaming.\n",
    "It's a dataset of tv show transcripts and their associated recaps, and for this demo we'll filter down to using only episodes\n",
    "from the popular US tv show called \"The Office\". This filtered dataset is useful for a few reasons:\n",
    "\n",
    "1. The input transcripts are quite long (>4k tokens) which means that generating a summary isn't trivial task.\n",
    "2. I'm a domain expert in this because I've watched all of the episodes of The Office many times.\n",
    " I'll be able to evaluate for myself how good the summary is, and I'll know if a model missed anything in its summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Requisites\n",
    "\n",
    "Before running this notebook, you need to have Lumigator running.You will need to ensure that you have both `OPENAI_API_KEY`, `DEEPSEEK_API_KEY`, and `MISTRAL_API_KEY` set in your environment variables. Then, run `make local-up` in order to build and have Lumigator listening. Now Lumigator should be ready for our experiment and be able to make requests to OpenAI, Mistral, and DeepSeek!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# First step, let's prepare the dataset!\n",
    "\n",
    "# First, grab the dataset off huggingface: https://huggingface.co/datasets/YuanPJ/summ_screen\n",
    "ds = load_dataset(\"YuanPJ/summ_screen\", \"fd\")[\"test\"]\n",
    "# filter for only examples which contain \"Gilmore_Girls\" in the File Name\n",
    "ds = ds.filter(lambda x: \"The_Office\" in x[\"File Name\"])\n",
    "\n",
    "# Now let's prepare it for Lumigator upload. We need to rename some columns and delete the rest\n",
    "# rename the column \"input\" to \"text\" and \"output\" to \"ground_truth\". This is what Lumigator expects\n",
    "ds = ds.rename_column(\"Transcript\", \"examples\")\n",
    "ds = ds.rename_column(\"Recap\", \"ground_truth\")\n",
    "\n",
    "# remove all columns except \"text\" and \"ground_truth\"\n",
    "columns_list = ds.column_names\n",
    "columns_list.remove(\"examples\")\n",
    "columns_list.remove(\"ground_truth\")\n",
    "ds = ds.remove_columns(columns_list)\n",
    "\n",
    "print(f\"The filtered test split contains {len(ds)} examples.\")\n",
    "# convert ds to a csv and make it a string so we can upload it with the Lumigator API\n",
    "DS_OUTPUT = \"office_dataset.csv\"\n",
    "ds.to_csv(DS_OUTPUT)\n",
    "MAX_SAMPLES = 1  # This demo is only designed to run on example, to make visual comparison easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from time import sleep\n",
    "\n",
    "from lumigator_schemas.datasets import DatasetFormat\n",
    "from lumigator_schemas.experiments import GetExperimentResponse\n",
    "from lumigator_schemas.workflows import WorkflowCreateRequest, WorkflowStatus\n",
    "from lumigator_sdk.lumigator import LumigatorClient\n",
    "\n",
    "\n",
    "def wait_for_all_workflows(lumi_client_int: LumigatorClient, experiment_id: str) -> GetExperimentResponse:\n",
    "    \"\"\"Wait for an experiment to complete.\"\"\"\n",
    "    still_running = True\n",
    "    while still_running:\n",
    "        still_running = False\n",
    "        experiment_details = lumi_client_int.experiments.get_experiment(experiment_id)\n",
    "        still_running_workflows = []\n",
    "        for workflow in experiment_details.workflows:\n",
    "            if workflow.status not in [WorkflowStatus.SUCCEEDED, WorkflowStatus.FAILED]:\n",
    "                still_running_workflows.append(workflow.name)\n",
    "        if still_running_workflows:\n",
    "            still_running = True\n",
    "            print(f\"Waiting for workflows {still_running_workflows} to complete\")\n",
    "            sleep(10)\n",
    "    return experiment_details\n",
    "\n",
    "\n",
    "# Time to connect up to the Lumigator client!\n",
    "LUMI_HOST = \"localhost:8000\"\n",
    "client = LumigatorClient(api_host=LUMI_HOST)\n",
    "\n",
    "# Upload that file that we created earlier\n",
    "with Path.open(DS_OUTPUT) as file:\n",
    "    data = file.read()\n",
    "dataset_response = client.datasets.create_dataset(dataset=data, format=DatasetFormat.JOB)\n",
    "dataset_id = dataset_response.id\n",
    "print(f\"Dataset uploaded and has ID: {dataset_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now time to create an experiment in Lumigator! This is a container for all the workflows we'll run\n",
    "from lumigator_schemas.experiments import ExperimentCreate\n",
    "\n",
    "request = ExperimentCreate(\n",
    "    name=\"The Office Summarization\",\n",
    "    description=\"Bears, Beets, Battlestar Galactica\",\n",
    "    dataset=dataset_id,\n",
    "    max_samples=MAX_SAMPLES,\n",
    ")\n",
    "experiment_response = client.experiments.create_experiment(request)\n",
    "experiment_id = experiment_response.id\n",
    "print(f\"Experiment created and has ID: {experiment_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lumigator_schemas.workflows import WorkflowDetailsResponse\n",
    "\n",
    "\n",
    "# Wait till the workflow is done\n",
    "def get_workflow_results(workflow: WorkflowDetailsResponse):\n",
    "    response = requests.get(workflow.artifacts_download_url)\n",
    "    result = response.json()\n",
    "    results = {\n",
    "        \"rouge2\": round(result[\"metrics\"][\"rouge\"][\"rouge2_mean\"], 2),\n",
    "        \"bertscore\": round(result[\"metrics\"][\"bertscore\"][\"f1_mean\"], 2),\n",
    "        \"meteor\": round(result[\"metrics\"][\"meteor\"][\"meteor_mean\"], 2),\n",
    "        \"predictions\": result[\"artifacts\"][\"predictions\"],\n",
    "        \"ground_truth\": result[\"artifacts\"][\"ground_truth\"],\n",
    "        \"examples\": result[\"artifacts\"][\"examples\"],\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run the Deepseek R1 https://api-docs.deepseek.com/quick_start/pricing\n",
    "request = WorkflowCreateRequest(\n",
    "    name=\"Deepseek R1\",\n",
    "    description=\"Summarize with Deepseek R-1\",\n",
    "    model=\"deepseek/deepseek-reasoner\",\n",
    "    dataset=dataset_id,\n",
    "    experiment_id=experiment_id,\n",
    "    max_samples=MAX_SAMPLES,\n",
    ")\n",
    "client.workflows.create_workflow(request).model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run the Deepseek V3 https://api-docs.deepseek.com/quick_start/pricing\n",
    "request = WorkflowCreateRequest(\n",
    "    name=\"Deepseek V3\",\n",
    "    description=\"Summarize with Deepseek V3\",\n",
    "    model=\"deepseek/deepseek-chat\",\n",
    "    dataset=dataset_id,\n",
    "    experiment_id=experiment_id,\n",
    "    max_samples=MAX_SAMPLES,\n",
    ")\n",
    "client.workflows.create_workflow(request).model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run the Deepseek V3 https://api-docs.deepseek.com/quick_start/pricing\n",
    "request = WorkflowCreateRequest(\n",
    "    name=\"Mistral Large\",\n",
    "    description=\"Summarize with Mistral Lage\",\n",
    "    model=\"mistral/mistral-large-latest\",\n",
    "    dataset=dataset_id,\n",
    "    experiment_id=experiment_id,\n",
    "    max_samples=MAX_SAMPLES,\n",
    ")\n",
    "client.workflows.create_workflow(request).model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's run the same thing, but with o3-mini\n",
    "request = WorkflowCreateRequest(\n",
    "    name=\"OpenAI o1\",\n",
    "    description=\"Summarize with o1\",\n",
    "    model=\"openai/o1\",\n",
    "    dataset=dataset_id,\n",
    "    experiment_id=experiment_id,\n",
    "    max_samples=MAX_SAMPLES,\n",
    ")\n",
    "client.workflows.create_workflow(request).model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's run the same thing, but with GPT-40\n",
    "request = WorkflowCreateRequest(\n",
    "    name=\"OpenAI o3-mini\",\n",
    "    description=\"Summarize with o3-mini\",\n",
    "    model=\"openai/o3-mini\",\n",
    "    dataset=dataset_id,\n",
    "    experiment_id=experiment_id,\n",
    "    max_samples=MAX_SAMPLES,\n",
    ")\n",
    "client.workflows.create_workflow(request).model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's run the same thing, but with GPT-40\n",
    "request = WorkflowCreateRequest(\n",
    "    name=\"Llamafile Mistral\",\n",
    "    description=\"Summarize with Q4 Llamafile\",\n",
    "    model=\"openai/scoopdewhoop\",\n",
    "    base_url=\"http://localhost:8080/v1\",\n",
    "    dataset=dataset_id,\n",
    "    experiment_id=experiment_id,\n",
    "    max_samples=MAX_SAMPLES,\n",
    ")\n",
    "client.workflows.create_workflow(request).model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "experiment = wait_for_all_workflows(client, experiment_id)\n",
    "print(f\"Experiment: {experiment.name}\")\n",
    "# create a table with the results\n",
    "table = pd.DataFrame()\n",
    "for workflow in experiment.workflows:\n",
    "    print(f\"--------{workflow.name}--------\")\n",
    "    print(f\"Desc: {workflow.description}\")\n",
    "    if workflow.status == WorkflowStatus.SUCCEEDED:\n",
    "        results = get_workflow_results(workflow)\n",
    "        print(f\"ROUGE2: {results['rouge2']}\")\n",
    "        print(f\"BERTScore: {results['bertscore']}\")\n",
    "        print(f\"METEOR: {results['meteor']}\")\n",
    "        for idx, prediction in enumerate(results[\"predictions\"]):\n",
    "            hypo = prediction[\"choices\"][0][\"message\"][\"content\"]\n",
    "            comp_tok = prediction[\"usage\"][\"completion_tokens\"]\n",
    "            prompt_tok = prediction[\"usage\"][\"prompt_tokens\"]\n",
    "            reasoning_tok = prediction[\"usage\"][\"completion_tokens_details\"]\n",
    "            if reasoning_tok:\n",
    "                comp_tok = comp_tok - reasoning_tok[\"reasoning_tokens\"]\n",
    "            print(f\"Example {idx}\")\n",
    "            print(f\"Prediction: \\n{hypo}\\n\")\n",
    "            print(f\"Completion Tokens: {comp_tok}\")\n",
    "            print(f\"Prompt Tokens: {prompt_tok}\")\n",
    "            print(f\"Reasoning Tokens: {reasoning_tok}\")\n",
    "            # add a new row to the table\n",
    "            table = pd.concat(\n",
    "                [\n",
    "                    table,\n",
    "                    pd.DataFrame(\n",
    "                        {\n",
    "                            \"Model\": workflow.name,\n",
    "                            \"ROUGE2\": results[\"rouge2\"],\n",
    "                            \"BERTScore\": results[\"bertscore\"],\n",
    "                            \"METEOR\": results[\"meteor\"],\n",
    "                            \"Example\": idx,\n",
    "                            \"Prediction\": hypo,\n",
    "                            \"Tokens\": comp_tok,\n",
    "                            \"Prompt Tokens\": prompt_tok,\n",
    "                            \"Reasoning Tokens\": reasoning_tok,\n",
    "                        },\n",
    "                        index=[0],\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "    else:\n",
    "        print(f\"Workflow {workflow.id} failed: deleting the workflow.\")\n",
    "        client.workflows.delete_workflow(workflow.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a table the prints out all the automatic metrics and numbers for easy comparison\n",
    "print(table[[\"Model\", \"ROUGE2\", \"BERTScore\", \"METEOR\", \"Tokens\"]].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Although we can't make any confident conclusions about which model is better overall (because all models may have had data about The Office in their training mix), DeepSeek models look to be competitive with OpenAI's and Mistral's for this summarization task. What other summarization tasks might be interesting to compare using Lumigator?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
