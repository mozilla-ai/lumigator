{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3373f2c7-da8e-49eb-8300-25ae4a00fdaa",
   "metadata": {},
   "source": [
    "# Lumigator from [Mozilla.ai](https://www.mozilla.ai/) üêä ü¶ä"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ba77ea-aae4-4c55-ad18-1b665836532c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "+ Working with Jupyter Notebooks\n",
    "+ Lumigator Platform üêä and Machine Learning Overview  \n",
    "+ Understanding Machine Learning Workflows \n",
    "+ Thunderbird Ground Truth Dataset Walkthrough\n",
    "+ Selection of models \n",
    "    + 1 encoder/decoder (BART) \n",
    "    + 2 decoder (Mistral and GPT-4o) to evaluate against GT\n",
    "+ Run evaluation experiment\n",
    "+ Discuss results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e41dab1",
   "metadata": {},
   "source": [
    "## Jupyter Walkthrough\n",
    "\n",
    "[Jupyter Notebooks](https://jupyter-notebook.readthedocs.io/en/stable/) are an executable code/text environment for (usually) Python code. Our Jupyter environment is in JupyterHub. To work with Jupyter, click \"run cell\" to run the code and see results below the cell you're currently running. Cells are executed sequentially. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf10cbb-f6a0-4a24-9d6d-30fac1f7d996",
   "metadata": {},
   "source": [
    "# Running cells \n",
    "To run a cell, press the \"play\" icon in the top bar (you can also hit Shift+Enter to run and proceed to the following cell).\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mozilla-ai/lumigator/main/docs/assets/running.png\" alt=\"drawing\" width=\"400\"/>\n",
    "\n",
    "Your files are located on the left-hand side. They'll be saved for the duration of our session, but if you'd like to keep them, make sure to download them. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mozilla-ai/lumigator/main/docs/assets/files.png\" alt=\"drawing\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb7563e-3ab3-486f-b45a-7bd0c47fbcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets' try running some code!\n",
    "# You can see the output below!\n",
    "print(\"Welcome to Lumigator!üêä\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecaa846",
   "metadata": {},
   "source": [
    "## Machine learning glossary\n",
    "\n",
    "Some terms you'll hear us using throughout the session: \n",
    "\n",
    "+ **Machine learning** - The process of creating a model that learns from data\n",
    "+ **Dataset** - Data used to train models and evaluate their performance\n",
    "+ **LLM** - Large language model, [a text-based model that performs next-word predictions](https://www.nvidia.com/en-us/glossary/large-language-models/) \n",
    "+ **Tokens** - Words broken up into pieces to be used in an LLM \n",
    "+ **Inference** - The process of getting a prediction from a large language model \n",
    "+ **Embeddings** - Numerical representations of text generated by modeling \n",
    "+ **Encoder-decoder models** - A neural network architecture comprised of two neural networks, an encoder that takes the input vectors from our data and creates an embedding of a fixed length, and a decoder, also a neural network, which takes the embeddings encoded as input and generates a static set of outputs such as translated text or a text summary\n",
    "+ **Decoder-only models** - Given a fixed input prompt, uses its representation to generate a sequence of words one at a time, with each word being conditioned on the ones generated previously\n",
    "+ **Task** - Machine learning tasks to fit a specific model type, including translation, summarization, completion, etc. \n",
    "+ **Ground truth** - Information that has been evaluated to be true by humans (or LLMs, in some cases) to be correct, that we can use as a point of comparison for our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709af016-5f3f-4552-8a8a-e731e60eb89d",
   "metadata": {},
   "source": [
    "The process of **machine learning** is the process of creating a mathematical model that tries to approximate the world. A **machine learning model** is a set of instructions for generating a given\n",
    "output from data. The instructions are learned from the features of the input data itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545e65e6-4e05-4aac-9bd5-c8521fbcb9e2",
   "metadata": {},
   "source": [
    "Within the universe of modeling approaches, there are **supervised** and **unsupervised** approaches, as well as reinforcement learning. \n",
    "Language modeling of the kind we do with LLMs falls in the realm of neural network approaches. \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mozilla-ai/lumigator/main/docs/assets/ml_family.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f67ff2a",
   "metadata": {},
   "source": [
    "## How do LLMs work? \n",
    "\n",
    "There are many different kinds of LLMs and many different kinds of architectures. For our evaluations, we use two different kinds:\n",
    "\n",
    "+ **Encoder/Decoder** - BART model, Converts input data into a fixed-size representation (similar to encoder models). These models are trained first to generate text into numerical representations, then to output text based on those numerical representations. They're good for synthesis as opposed to generation. \n",
    "+ **Decoder-only** - most GPT-family models like Mistral, GPT, and others we'll be working with.  These models are pre-trained with text data in an autoregressive manner, for next-token prediction given previous tokens.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8139990",
   "metadata": {},
   "source": [
    "## LLM Workflows\n",
    "\n",
    "1. Pre-train the model itself and generate a model artifact\n",
    "2. Generate ground truth for our business use-case\n",
    "3. Pick several models we'd like to use to evaluate\n",
    "4. Run an evaluation loop consisting of looking at the ground truth in comparison to model results\n",
    "5. Analyze our evaluations.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mozilla-ai/lumigator/main/docs/assets/llm_steps.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c44d8a1",
   "metadata": {},
   "source": [
    "Lumigator on a technical level is a **Python-based FastAPI web app** with services that run **jobs and deployments on a Ray cluster** which can be run either locally or in the cloud, depending on your computer specs. Results are stored in **Postgres**. Larger models loaded from HuggingFace require GPUs.  \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mozilla-ai/lumigator/main/docs/assets/platform.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed38e49",
   "metadata": {},
   "source": [
    "What is Ray? [A distributed runtime for Python programs](https://github.com/ray-project/ray) that includes a Core library with primitives (Tasks, Actors, and objects) and a suite of ML libraries (Tune, Serve) that allow to build components of the machine learning model workflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838d2fd4",
   "metadata": {},
   "source": [
    "## Nota bene: Machine learning is alchemy\n",
    "\n",
    "When we think of traditional software application workflows, we think of an example such as adding a button. We can clearly test that we've added a blue button to our application, and that it works correctly. Machine learning is not like this! It involves a lot of experimentation, tweaking of hyperparameters and prompts and trying different models. Expect for the process to be imperfect, with many iterative loops. Luckily, Lumigator helps take away the uncertainty of at least model selection :)\n",
    "\n",
    "> There‚Äôs a self-congratulatory feeling in the air. We say things like ‚Äúmachine learning is the new electricity‚Äù. I‚Äôd like to offer an alternative metaphor: machine learning has become alchemy. - [Ben Recht and Ali Rahimi](https://archives.argmin.net/2017/12/05/kitchen-sinks/)\n",
    "\n",
    "Ultimately, the final conclusion of whether a model is good is if humans think it's good. \n",
    "\n",
    "With that in mind, let's dive into setting up experiments with Lumigator to test our models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbdf25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a library of utility functions that will help us connect to the Lumigator API\n",
    "# Let's take a second to walk through them \n",
    "\n",
    "import lumigator_demo as ld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7afa9ec9b5f866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages we need to work with data \n",
    "# python standard libraries\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Random string generator\n",
    "import random\n",
    "import string\n",
    "import shortuuid\n",
    "\n",
    "# third-party libraries\n",
    "from datasets import load_dataset\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# wrap columns for inspection\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "# stylesheet for visibility\n",
    "plt.style.use(\"fast\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1cd443f885af1d",
   "metadata": {},
   "source": [
    "# Understanding the Lumigator App and API \n",
    "\n",
    "The app itself consists of an API, which you can access and test out methods in the [OpenAPI spec](https://swagger.io/specification/), at the platform URL, under docs.\n",
    "If you are running Lumigator as a local installation, you can directly access the API at [this URL](http://localhost/docs#/).\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mozilla-ai/lumigator/main/docs/assets/openapi.png\" alt=\"drawing\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd6f7aa716833f7",
   "metadata": {},
   "source": [
    "Large language models today are consumed in one of several ways:\n",
    "\n",
    "+ As **API endpoints** for proprietary models hosted by OpenAI, Anthropic, or major cloud providers\n",
    "+ As **model artifacts** downloaded from HuggingFace‚Äôs Model Hub and/or trained/fine-tuned using HuggingFace libraries and hosted on local storage\n",
    "+ As model artifacts available in a format optimized for **local inference**, typically GGUF, and accessed via applications like llama.cpp or ollama\n",
    "+ As ONNX, a format which optimizes sharing between backend ML frameworks\n",
    "\n",
    "We use API endpoints and local storage in Lumigator. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc80b3a9340e8502",
   "metadata": {},
   "source": [
    "\n",
    "We currently have 5 key endpoints on the platform. \n",
    "\n",
    "+ `Health` - Status of the application, running status of jobs and deployments. \n",
    "+ `Datasets` - Data that we add to the platform for evaluation. We can upload, delete, and save different data in the platform. - We'll use this to save our ground truth and experiment data\n",
    "+ `Experiments` - Our actual evaluation experiments. We can list all previous evaluations, create new ones, and get their results.\n",
    "+ `Groundtruth` - Running Ray-serve deployments with locally-hosted models\n",
    "+ `Completions` - Access to external APIs such as Mistral and OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a28c048ea00706",
   "metadata": {},
   "source": [
    "## Model Task: Summarization\n",
    "\n",
    "The task we'll be working with is **summarization**, aka we want to generate a summary of our text. \n",
    "\n",
    "In our business case, which is to create summaries of conversation threads, much as you might see in Slack or an email chain, the models need to be able to extract key information from those threads while still being able to accept a large context window to capture the entire conversation history. \n",
    "\n",
    "We identified that it is far more valuable to conduct **abstractive** summaries, or summaries that identify important sections in the text and generate highlights,  rather than **extractive** ones, which pick a subset of sentences and add them together for our use cases since the final interface will be natural language. We want the summary results not to need to be interpreted from often incoherent text snippets produced by extractive summaries. \n",
    "\n",
    "For more on summarization as a use-case, [see our blog post here.](https://blog.mozilla.ai/on-model-selection-for-text-summarization/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33e80b64bad99e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Ground Truth for Models\n",
    "\n",
    "The term ground truth comes from geology and geospatial sciences, where actual information was collected on the ground to validate data acquired through remote sensing, such as satellite imagery or aerial photography. Since then, the concept has been adopted in other fields, particularly in machine learning and artificial intelligence, to refer to the accurate, real-world data used for training and testing models. \n",
    "\n",
    "The **best ground truth is human-generated** but building it is a very expensive task.\n",
    "One recent trend is to rely on large language models but (as you will see later) they have their own pitfalls.\n",
    "An intermediate approach uses different LLMs to provide ground truth \"candidates\" which are then subject to human pairwise evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d102a0b4515342",
   "metadata": {},
   "source": [
    "## Our Input data\n",
    "\n",
    "The data we'll be using in this walkthrough comes from [DialogSum](https://github.com/cylnlp/DialogSum), a large-scale\n",
    "labeled dialogue summarization dataset which comes with ground truth provided by human annotators.\n",
    "\n",
    "For the sake of explanation, in the next section we will show how Lumigator allows one to generate a ground truth candidate with the help of a language model. However, in the following evaluation section we will compare against the original, human-provided, annotations.\n",
    "\n",
    "Here follows a brief description of DialogSum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65af6945-bd5d-4506-af3c-144683779d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataset is available at https://huggingface.co/datasets/knkarthick/dialogsum\n",
    "# and can be directly downloaded with the `load_dataset` method\n",
    "dataset='knkarthick/dialogsum'\n",
    "ds = load_dataset(dataset, split='validation')\n",
    "df = ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb05409-aad2-42e2-bc23-743575ad924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine a single sample \n",
    "df['dialogue'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac02084ba6b6fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a function to do some simple character counts for model input\n",
    "df['char_count'] = df['dialogue'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90ceb9398ae3ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect our data\n",
    "df.head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359677f7f12bcefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show statistics about characters count\n",
    "df['char_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116e1127c3846883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate plot of character counts\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.hist(df['char_count'], bins=30)\n",
    "ax.set_xlabel('Character Count')\n",
    "ax.set_ylabel('Frequency')\n",
    "\n",
    "stats = df['char_count'].describe().apply(lambda x: f\"{x:.0f}\")\n",
    "\n",
    "# Add text boxes for statistics\n",
    "plt.text(1.05, 0.95, stats.to_string(), \n",
    "         transform=ax.transAxes, verticalalignment='top')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "fig.subplots_adjust(right=0.75)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc4643b-d6b4-40e8-97d1-c6b9cb50d7ae",
   "metadata": {},
   "source": [
    "## Ground Truth Generation with Mistral\n",
    "\n",
    "In the following we will use [Mistral API](https://docs.mistral.ai/api/) to generate candidate ground truth. Note that for the following code to work, you need to have a valid mistral API key (no worries if you don't, the next section shows another methods which relies on an opensource model running locally on your computer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f676203834ab1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Ground Truth Generation with Mistral\n",
    "\n",
    "mistral_responses = []\n",
    "\n",
    "for sample in df['dialogue'][0:10]:\n",
    "    res = ld.get_mistral_ground_truth(sample)\n",
    "    print(f\"Mistral Summary:\", res)\n",
    "    mistral_responses.append((sample, res['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0528d0e-ffd9-422d-ac54-9d49403b7e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a result set we can look at\n",
    "mistral_results_df = pd.DataFrame(mistral_responses, columns=['examples', 'mistral_response'])\n",
    "mistral_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26242adc-c1e1-4f30-8397-4e7cd935f0f7",
   "metadata": {},
   "source": [
    "## Ground Truth Generation with a local model\n",
    "\n",
    "In this section we will use a local model to generate ground truth.\n",
    "\n",
    "To do this, you will spin up a new _ray deployment_ from Lumigator's API. You can do it manually by connecting to the API and submitting the following JSON to the `/api/v1/ground-truth/deployments` endpoint:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"num_gpus\": 0,\n",
    "  \"num_cpus\": 1,\n",
    "  \"num_replicas\": 1\n",
    "}\n",
    "```\n",
    "... or you can copy and paste the following into your terminal to send the same request to the API via `curl`:\n",
    "\n",
    "```\n",
    "curl -X 'POST' \\\n",
    "  'http://localhost/api/v1/ground-truth/deployments' \\\n",
    "  -H 'accept: application/json' \\\n",
    "  -H 'Content-Type: application/json' \\\n",
    "  -d '{\n",
    "  \"num_gpus\": 0,\n",
    "  \"num_cpus\": 1,\n",
    "  \"num_replicas\": 1\n",
    "}'\n",
    "```\n",
    "\n",
    "After this, you can run the following cell to see information about your new deployment. Also note that you can get more details about it on your [ray dashboard](http://localhost:8265/#/actors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55ca94955da27ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at all available deployments\n",
    "ld.get_deployments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcb0a6e78337093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Ground Truth Generation with BART\n",
    "\n",
    "deployment_id = ld.get_summarizer_deployment_id()\n",
    "\n",
    "bart_responses = []\n",
    "\n",
    "for prompt in df['dialogue'][0:10]:\n",
    "    response = ld.get_bart_ground_truth(deployment_id, prompt)\n",
    "    response_dict = json.loads(response.text)\n",
    "    results = response_dict.get('deployment_response', {}).get('result', 'No result found')\n",
    "    print(\"BART:\", results)\n",
    "    bart_responses.append((prompt, results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2928247553e1cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_results_df = pd.DataFrame(bart_responses, columns=['examples', 'bart_response'])\n",
    "bart_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557e649335fe060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine results and examine multiple versions of ground truth\n",
    "merged_df = pd.merge(bart_results_df, mistral_results_df, on='examples', how='outer')\n",
    "merged_df.to_csv('ground_truth.csv', index=False)\n",
    "merged_df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378251f3-4145-4fa6-b933-564a91101c44",
   "metadata": {},
   "source": [
    "## Save and upload datasets\n",
    "\n",
    "Now that you have seen the different available options to generate ground truth, let us save all datasets and make them available to lumigator for further experiments.\n",
    "\n",
    "For each example (dialogsum original dataset, bart-generated gt, mistral-generated gt) we will perform the following operations:\n",
    "\n",
    "1. Make sure that the two main fields (original text and ground truth) are called `examples` and `ground_truth`, which are the names internally used by Lumigator to refer to them, and save the datasets as CSV files.\n",
    "\n",
    "2. Make the dataset available to Lumigator with the `dataset_upload` method. If you are running locally the term \"upload\" might not make much sense to you, but it does if you think about it as a system which can work in a distributed fashion: with this method, your data is accessed locally or on a remote S3 store depending on the setup, in a way which is consistent and transparent to users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0f300c-9651-473b-aca9-1f147a3f2f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.remove_columns([\"id\", \"topic\"])\n",
    "ds = ds.rename_column(\"dialogue\", \"examples\")\n",
    "ds = ds.rename_column(\"summary\", \"ground_truth\")\n",
    "dataset_name = \"dialogsum_converted.csv\"\n",
    "ds.to_csv(dataset_name)\n",
    "res = ld.dataset_upload(dataset_name)\n",
    "dialogsum_dataset_id = json.loads(res.content)['id']\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad81836193f9ed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_results_df = bart_results_df.rename(columns={\"bart_response\": \"ground_truth\"})\n",
    "bart_results_df.to_csv('bart_ground_truth.csv', index=False)\n",
    "ld.dataset_upload(\"bart_ground_truth.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4115d37099e35f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_results_df = mistral_results_df.rename(columns={\"mistral_response\": \"ground_truth\"})\n",
    "mistral_results_df.to_csv('mistral_ground_truth.csv', index=False)\n",
    "ld.dataset_upload(\"mistral_ground_truth.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e5b2701933bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And let's check that data loaded\n",
    "ld.get_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3019b45b211087",
   "metadata": {},
   "source": [
    "# Experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a152e1e084e7b64",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f2fe15f29bdc4f",
   "metadata": {},
   "source": [
    "After generating the ground truth (either manually or with the aid of some models) and uploading the dataset to lumigator, we are ready to start evaluating models on it.\n",
    "\n",
    "Note that when you uploaded your datasets you were returned some information that included a `dataset_id`. This is a unique identifier to your own dataset that you can reuse across different experiment. We added `dialogsum_dataset_id` as the default identifier to use, but feel free to add a custom one if you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98f8a6fdff48654",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = dialogsum_dataset_id\n",
    "\n",
    "# now look for the dataset on lumigator\n",
    "r = ld.dataset_info(dataset_id)\n",
    "dataset_name = json.loads(r.text)['filename']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51eb04dbc16b035",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "What you see below are different lists of models we have already tested for the summarization task.\n",
    "The `models` variable at the end provides you with a selection, but you can choose any combination of them:\n",
    "the default is a single local model (`facebook/bart-large-cnn`), but depending on your setup you can choose\n",
    "more and/or add different APIs.\n",
    "\n",
    "Note that different model types are specified with different prefixes:\n",
    "\n",
    "- `hf://` is used for HuggingFace models which are downloaded and ran as Ray jobs\n",
    "- `mistral://` is used for models which are accessed through the Mistral API\n",
    "- `oai://` is used for models which are accessed through an OpenAI-compatible API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3f8c76c37a2b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here follows a list of models we have tested for summarization:\n",
    "# feel free to add any of them in the \"models\" list below\n",
    "#\n",
    "# Encoder-Decoder models\n",
    "#    'hf://facebook/bart-large-cnn',\n",
    "#    'hf://mikeadimech/longformer-qmsum-meeting-summarization', \n",
    "#    'hf://mrm8488/t5-base-finetuned-summarize-news',\n",
    "#    'hf://Falconsai/text_summarization',\n",
    "#\n",
    "# Decoder models\n",
    "#    'mistral://open-mistral-7b',\n",
    "#\n",
    "# GPTs\n",
    "#    \"oai://gpt-4o-mini\",\n",
    "#    \"oai://gpt-4-turbo\",\n",
    "#    \"oai://gpt-3.5-turbo-0125\",\n",
    "#\n",
    "models = [\n",
    "    'hf://facebook/bart-large-cnn',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa55eee63016041",
   "metadata": {},
   "source": [
    "## Run Evaluations\n",
    "\n",
    "The following cell will start the actual model evaluations.\n",
    "Once you run it, new jobs will be submitted to ray (one for each model) and the outcomes of these submissions will be printed.\n",
    "Each evaluation job will first use the provided model to summarize each of the emails in the input dataset. After that, it will calculate a few metrics to evaluate how close the predicted summaries are to the ground truth provided in the dataset.\n",
    "\n",
    "Each job starts with a `created` status. While the job runs, you will be able to track its status by running the cell in the section **Track evaluation jobs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f344b6c4998e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this value to limit the evaluation to the first max_samples items (0=all)\n",
    "max_samples = 10\n",
    "# team_name is a way to group jobs together under the same namespace, feel free to customize it\n",
    "team_name = \"lumigator_enthusiasts\"\n",
    "\n",
    "responses = []\n",
    "for model in models:\n",
    "    descr = f\"Testing {model} summarization model on {dataset_name}\"\n",
    "    responses.append(ld.experiments_submit(model, team_name, descr, dataset_id, max_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f787190-4581-4c26-beca-0260303a68bd",
   "metadata": {},
   "source": [
    "### Track evaluation jobs\n",
    "\n",
    "Run the following to track your evaluation jobs.\n",
    "\n",
    "- *NOTE*: you won't be able to run other cells while this one is running. However, you can interrupt it whenever you want by clicking on the \"stop\" button above and run it at a later time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2858bcea-6fcb-4441-b56e-5d89502f10e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_ids = [ld.get_resource_id(r) for r in responses]\n",
    "\n",
    "wip = ld.show_experiment_statuses(job_ids)\n",
    "while wip == True:\n",
    "    time.sleep(5)\n",
    "    clear_output()\n",
    "    wip=ld.show_experiment_statuses(job_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d8cbf3-e44f-4f1e-b520-3f6fac147fca",
   "metadata": {},
   "source": [
    "## Show evaluation results\n",
    "\n",
    "Once all evaluations are completed, their results will be stored on our platform and available for download. \n",
    "You can download them individually with the command\n",
    "\n",
    "```python\n",
    "ld.experiments_result_download(job_id)\n",
    "```\n",
    "\n",
    "The following cell iterates on all your job ids, downloads results from each, and builds a table comparing different metrics for each model.\n",
    "The metrics we use to evaluate are ROUGE, METEOR, and BERT score. They all measure similarity between predicted summaries and those provided with the ground truth, but each of them focuses on different aspects. The image below shows their main characteristics and the tradeoffs between their flexibility and their computational cost.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mozilla-ai/lumigator/main/docs/assets/metrics.png\" alt=\"drawing\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8709b8-59ba-4a9c-81bd-59b2fc612f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after the jobs complete, gather evaluation results\n",
    "eval_results = []\n",
    "for job_id in job_ids:\n",
    "    eval_results.append(ld.experiments_result_download(job_id))\n",
    "\n",
    "# convert results into a pandas dataframe\n",
    "ld.eval_results_to_table(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b450ce",
   "metadata": {},
   "source": [
    "## Analysis of Evaluation Results\n",
    "\n",
    "The tablel above is just a summary of all the evaluation results.\n",
    "The `eval_results` object contains way more details from which you'll be able to get a few more insights in the following cells.\n",
    "\n",
    "### Direct access to all data\n",
    "\n",
    "The following cell shows you the kind of information that's available in each of the `eval_results` elements. This information is nested at different depth levels. You can access each using the `get_nested_value` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadbdaec-a1e9-4731-bfdd-b471e6a0fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_results is a list holding information for each of the models you defined before\n",
    "# for each element, you can access different metrics, time performance, and predictions\n",
    "eval_results[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec4d37e-4750-404e-b40c-3778b8196d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how much time it took for a model to summarize all the input samples\n",
    "ld.get_nested_value(eval_results[0], \"summarization_time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a73261-e5a4-46a6-9b7c-1bc5e62dc1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see all the bertscore data\n",
    "ld.get_nested_value(eval_results[0], \"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6e8285-f407-4a5f-acda-3400554c3d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see mean bert precision\n",
    "ld.get_nested_value(eval_results[0], \"bertscore/precision_mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133c21f4-e61a-4f07-975f-052ccdead1da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
