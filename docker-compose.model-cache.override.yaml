services:
  inference-model:
    build:
      context: .
      dockerfile: cache/Dockerfile.model-inference
    platform: linux/${ARCH}
    volumes:
      - ${HF_HOME}:/home/ray/.cache/huggingface
    environment:
      - MODELS_CACHED
    profiles:
      - local

  ray:
    depends_on:
      inference-model:
        condition: service_completed_successfully
