# Bring Your Own Local LLMs
Previously we saw how to use Lumigator with models on Hugging Face as well as via APIs hosted by [Open AI and Mistral](../user-guides/inference.md#model-specification). However, sometimes it may be advantageous to run models locally as a more cost-effective solution or when dealing with sensitive data or during the early stages of experimentation. Lumigator supports running inference and evaluation on locally models hosted through [Llamafile](https://github.com/Mozilla-Ocho/llamafile), [Ollama](https://ollama.com/search) and [vLLM](https://docs.vllm.ai/en/latest/), which enable locally hosting several LLMs. This guide will walk you through the process of running evalutation on any local model that you bring from these providers (assuming your machine meets the necessary hardware requiremnts).
