# Datasets

A dataset is a collection of data points (or samples) used for training, testing, or evaluating machine learning models.

An **evaluation** dataset specifically assesses a model's performance on a particular task.

## Why bother creating your own

Unlike generic, open source benchmarks, a curated dataset can reflect the nuances, vocabulary, and challenges unique to the use case. Intuitively, summarizing patient information will require a different vocabulary than summarizing a film. Even within the same domain, some models may be enough to handle short technical messages, whereas they might struggle with long technical papers.

Would that model also work for you better than a generic LLM? Would you really lose performance if you used a smaller model? Those are questions Lumigator can help you answer this for your use case. They emphasis here is on "for your use case"; for an LLM, data is what best defines a use case.

## What is a good dataset?

You can see it through the analogy of setting a good exam for a class. Will it cover the syllabus completely? Is it long enough to avoid the odd (un)lucky answer having too much weight? For each answer you include in your exame, do you know what the ideal answer would be, to compare students' answers to it?

## Content

At its simplest, an evaluation dataset for a specific task should contain the following key components:

* Input text or examples: These are the samples that the model will process; e.g. each individual text from your domain that a model should summarize.
* Ground truth: Answers that you would deem correct for each input. Ideally these are generated by expert humans.

Ideally, you could also add metadata in additional colums that detail the source, date of collection or level of difficulty of each sample. However, Lumigator does not use this additional metadata.


## I'm convinced, how do I prepare one?

Follow the guide on [Preparing your own evaluation dataset](../user-guides/prepare-evaluation-dataset.md).
